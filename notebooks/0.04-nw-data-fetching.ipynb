{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching: FJC and Congress.gov API\n",
    "\n",
    "This notebook is responsible for fetching and initially processing data from our primary sources:\n",
    "\n",
    "1. Federal Judicial Center (FJC) CSV and Excel files\n",
    "2. Congress.gov API judicial nomination data\n",
    "\n",
    "According to the project architecture, this notebook will:\n",
    "1. Download or use cached data from the FJC and Congress.gov API\n",
    "2. Perform minimal transformations to convert to dataframes\n",
    "3. Save the resulting dataframes to `data/raw` for further processing by downstream notebooks\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:32.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mProject root: /home/wsl2ubuntuuser/nomination_predictor\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mConfiguration loaded\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "# Add the project root to the path so we can import our modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from nomination_predictor.congress_api import CongressAPIClient\n",
    "\n",
    "# Setup logging\n",
    "logger.remove()  # Remove default handler\n",
    "logger.add(sys.stderr, format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{function}</cyan> - <level>{message}</level>\", level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Federal Judicial Center (FJC) Data\n",
    "\n",
    "The FJC data is our canonical source for judicial seat timelines, judge demographics, and nomination failures.\n",
    "\n",
    "### Check if FJC data exists or download if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mensure_fjc_data_files\u001b[0m - \u001b[1mEnsuring FJC data files are available\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required FJC data files are available in /home/wsl2ubuntuuser/nomination_predictor/data/external\n"
     ]
    }
   ],
   "source": [
    "# Check if required FJC data files exist and download any missing ones\n",
    "from nomination_predictor.config import EXTERNAL_DATA_DIR\n",
    "from nomination_predictor.fjc_data import (REQUIRED_FJC_FILES,\n",
    "                                           ensure_fjc_data_files,\n",
    "                                           load_fjc_data)\n",
    "\n",
    "# Check for missing files and download them if needed\n",
    "downloaded, failed = ensure_fjc_data_files()\n",
    "\n",
    "# Report status\n",
    "if downloaded:\n",
    "    print(f\"✓ Downloaded {len(downloaded)} previously missing files: {', '.join(downloaded)}\")\n",
    "if failed:\n",
    "    print(f\"❌ Failed to download {len(failed)} files: {', '.join(failed)}\")\n",
    "    \n",
    "# Also report on which files are present\n",
    "present_files = [f for f in REQUIRED_FJC_FILES if (EXTERNAL_DATA_DIR / f).exists()]\n",
    "if len(present_files) == len(REQUIRED_FJC_FILES):\n",
    "    print(f\"✓ All required FJC data files are available in {EXTERNAL_DATA_DIR}\")\n",
    "else:\n",
    "    missing = set(REQUIRED_FJC_FILES) - set(present_files)\n",
    "    print(f\"⚠️ Still missing {len(missing)} required files: {', '.join(missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FJC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoading FJC data files\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mensure_fjc_data_files\u001b[0m - \u001b[1mEnsuring FJC data files are available\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: demographics.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded demographics data with 4022 records\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: education.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded education data with 8040 records\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: federal-judicial-service.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded federal_judicial_service data with 4720 records\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:32\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: judges.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded judges data with 4022 records\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: other-nominations-recess.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded other_nominations_recess data with 828 records\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: other-federal-judicial-service.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded other_federal_judicial_service data with 611 records\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_csv\u001b[0m - \u001b[1mLoading FJC data file: professional-career.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mload_fjc_data\u001b[0m - \u001b[1mLoaded professional_career data with 19003 records\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FJC data files:\n",
      "- demographics: 4022 records\n",
      "- education: 8040 records\n",
      "- federal_judicial_service: 4720 records\n",
      "- judges: 4022 records\n",
      "- other_nominations_recess: 828 records\n",
      "- other_federal_judicial_service: 611 records\n",
      "- professional_career: 19003 records\n"
     ]
    }
   ],
   "source": [
    "# Load all FJC data files (with auto-download enabled by default)\n",
    "fjc_data = load_fjc_data()\n",
    "\n",
    "# Access individual DataFrames\n",
    "print(f\"Loaded FJC data files:\")\n",
    "for key, df in fjc_data.items():\n",
    "    print(f\"- {key}: {len(df)} records\")\n",
    "\n",
    "# Store references to commonly used DataFrames for easier access\n",
    "judges_df = fjc_data.get('judges')\n",
    "demographics_df = fjc_data.get('demographics')\n",
    "education_df = fjc_data.get('education')\n",
    "federal_judicial_service_df = fjc_data.get('federal_judicial_service')\n",
    "other_nominations_recess_df = fjc_data.get('other_nominations_recess')\n",
    "other_federal_judicial_service_df = fjc_data.get('other_federal_judicial_service')\n",
    "professional_career_df = fjc_data.get('professional_career')\n",
    "\n",
    "# Create a dictionary of all FJC dataframes for easy iteration\n",
    "all_dataframes = {\n",
    "    'judges': judges_df,\n",
    "    'demographics': demographics_df,\n",
    "    'education': education_df,\n",
    "    'federal_judicial_service': federal_judicial_service_df,\n",
    "    'other_nominations_recess': other_nominations_recess_df,\n",
    "    'other_federal_judicial_service': other_federal_judicial_service_df,\n",
    "    'professional_career': professional_career_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a \"seat timeline\" inferred from FJC's data about when judges were in service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[1mBuilding seat timeline table\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CA11SR: 1998-06-07 > 1981-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CA11SR: 1993-11-11 > 1981-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CA11SR: 2001-11-15 > 1981-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CA11SR: 1982-10-27 > 1981-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:33\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CA11SR: 1987-08-22 > 1981-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:34\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CACDSR: 1974-03-09 > 1966-09-18\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:34\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CAFCSR: 1986-04-14 > 1982-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:34\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CAFCSR: 2007-10-28 > 1982-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:34\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mbuild_seat_timeline\u001b[0m - \u001b[33m\u001b[1mEditing a derived vacancy_date: Termination date > successor commission for seat CAFCSR: 1989-08-09 > 1982-10-01\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:37\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mbuild_and_validate_seat_timeline\u001b[0m - \u001b[1mSuccessfully built seat timeline with 4,720 records\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully built seat timeline with 4,720 records\n"
     ]
    }
   ],
   "source": [
    "from nomination_predictor.dataset import build_and_validate_seat_timeline\n",
    "\n",
    "try:\n",
    "    seat_timeline_df = build_and_validate_seat_timeline(federal_judicial_service_df)\n",
    "    print(f\"✅ Successfully built seat timeline with {len(seat_timeline_df):,} records\")\n",
    "    all_dataframes['seat_timeline'] = seat_timeline_df\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Congress.gov API Data\n",
    "\n",
    "The Congress.gov API provides detailed information about judicial nominations, including:\n",
    "- Nomination date\n",
    "- Nominee information\n",
    "- Confirmation status and date\n",
    "- Committee actions\n",
    "\n",
    "### Setup API Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Congress API key found in environment variables\n",
      "✓ Congress API client initialized\n"
     ]
    }
   ],
   "source": [
    "# Check if API key is available\n",
    "api_key = os.environ.get(\"CONGRESS_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"❌ Error: CONGRESS_API_KEY environment variable not set\")\n",
    "    print(\"Please set the CONGRESS_API_KEY environment variable to your Congress.gov API key\")\n",
    "    print(\"You can request an API key at: https://api.congress.gov/sign-up/\")\n",
    "else:\n",
    "    print(\"✓ Congress API key found in environment variables\")\n",
    "    # Initialize the API client\n",
    "    congress_client = CongressAPIClient(api_key)\n",
    "    print(\"✓ Congress API client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Judicial Nominations from Recent Congresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:38\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mfetch_judicial_nominations\u001b[0m - \u001b[1mFound cached nominations data at /home/wsl2ubuntuuser/nomination_predictor/data/raw/nominations.csv\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:38\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mfetch_judicial_nominations\u001b[0m - \u001b[1mLoaded 5746 nominations records from cache (retrieved from 2025-07-12 00:00 to 2025-07-12 00:00)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded 5746 nomination records\n"
     ]
    }
   ],
   "source": [
    "# Fetch judicial nominations from recent congresses\n",
    "# Congress numbering: 116th (2019-2021), 117th (2021-2023), 118th (2023-2025)\n",
    "# Import the new function\n",
    "import os\n",
    "\n",
    "from nomination_predictor.config import RAW_DATA_DIR\n",
    "from nomination_predictor.dataset import fetch_judicial_nominations\n",
    "\n",
    "# Define constants \n",
    "MOST_RECENT_CONGRESS_TERM_TO_GET = 118\n",
    "OLDEST_CONGRESS_TERM_TO_GET = 96\n",
    "\n",
    "# Define cache file path for nominations\n",
    "nominations_cache_file = os.path.join(RAW_DATA_DIR, \"nominations.csv\")\n",
    "\n",
    "# Fetch nominations with improved error handling\n",
    "nominations_df, success = fetch_judicial_nominations(\n",
    "    congress_client=congress_client,\n",
    "    most_recent_congress=MOST_RECENT_CONGRESS_TERM_TO_GET,\n",
    "    oldest_congress=OLDEST_CONGRESS_TERM_TO_GET,\n",
    "    auto_paginate=True,\n",
    "    cache_file=nominations_cache_file\n",
    ")\n",
    "\n",
    "# Critical validation - prevent proceeding if we don't have valid data\n",
    "if not success or len(nominations_df) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to retrieve valid nomination data. \"\n",
    "        \"Please check the logs for errors and fix any issues before continuing.\"\n",
    "    )\n",
    "\n",
    "# Add to all_dataframes collection if we have valid data\n",
    "all_dataframes['nominations'] = nominations_df\n",
    "\n",
    "print(f\"✓ Successfully loaded {len(nominations_df)} nomination records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          nomination  \\\n",
      "0  {'actions': {'count': 6, 'url': 'https://api.c...   \n",
      "1  {'actions': {'count': 6, 'url': 'https://api.c...   \n",
      "2  {'actions': {'count': 11, 'url': 'https://api....   \n",
      "3  {'actions': {'count': 14, 'url': 'https://api....   \n",
      "4  {'actions': {'count': 20, 'url': 'https://api....   \n",
      "\n",
      "                                             request retrieval_date  \\\n",
      "0  {'congress': '118', 'contentType': 'applicatio...     2025-07-12   \n",
      "1  {'congress': '118', 'contentType': 'applicatio...     2025-07-12   \n",
      "2  {'congress': '118', 'contentType': 'applicatio...     2025-07-12   \n",
      "3  {'congress': '118', 'contentType': 'applicatio...     2025-07-12   \n",
      "4  {'congress': '118', 'contentType': 'applicatio...     2025-07-12   \n",
      "\n",
      "   is_full_detail  \n",
      "0            True  \n",
      "1            True  \n",
      "2            True  \n",
      "3            True  \n",
      "4            True  \n"
     ]
    }
   ],
   "source": [
    "# Preview the nominations\n",
    "print(nominations_df.head())\n",
    "all_dataframes['nominations'] = nominations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch nominees for just-retrieved nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:38\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mextract_nominee_urls_from_nominations_df\u001b[0m - \u001b[1mProcessing 5746 nominations to extract nominee URLs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5671 nominations to retrieve nominee URLs for\n"
     ]
    }
   ],
   "source": [
    "from nomination_predictor.dataset import \\\n",
    "    extract_nominee_urls_from_nominations_df\n",
    "\n",
    "# Extract nominee URLs from the JSON-structured nominations DataFrame\n",
    "nominee_urls_df = extract_nominee_urls_from_nominations_df(nominations_df)\n",
    "nominee_urls = nominee_urls_df['nominee_url'].tolist()\n",
    "print(f\"Found {len(nominee_urls)} nominations to retrieve nominee URLs for\")\n",
    "\n",
    "# nominee_urls_df is neither intended nor necessary to be saved as a file;\n",
    "# it's simply a utility for another API-driven retrieval operation below.\n",
    "# Each row contains: citation, nominee_url, congress, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached nominees data at /home/wsl2ubuntuuser/nomination_predictor/data/raw/nominees.csv.  \n",
      "Loaded 5671 nominee records from cache; Loaded 5671 nominees records from cache (retrieved from 2025-07-13 00:00 to 2025-07-13 00:00)\n"
     ]
    }
   ],
   "source": [
    "from nomination_predictor.dataset import get_retrieval_date_range_message\n",
    "\n",
    "nominees_cache_file = os.path.join(RAW_DATA_DIR, \"nominees.csv\")\n",
    "\n",
    "# Check if we have cached data\n",
    "if os.path.exists(nominees_cache_file):\n",
    "    print(f\"Found cached nominees data at {nominees_cache_file}.  \")\n",
    "    nominees_df = pd.read_csv(nominees_cache_file)\n",
    "    print(f\"{get_retrieval_date_range_message(nominees_df, 'nominees')}\")\n",
    "elif 0 == len(nominee_urls):\n",
    "    print(\"⚠️ No nominee URLs found to download from\")\n",
    "else:\n",
    "    print(f\"Fetching nominee data for {len(nominee_urls)} nominations...\")\n",
    "\n",
    "    # Filter out records without nominee_url\n",
    "    nominees_data = congress_client.get_all_nominees_data(nominee_urls)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    nominees_df = pd.DataFrame(nominees_data)\n",
    "    print(f\"\\nTotal nominees retrieved: {len(nominees_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             nominee  \\\n",
      "0  {'nominees': [{'firstName': 'Nicholas', 'lastN...   \n",
      "1  {'nominees': [{'firstName': 'James', 'lastName...   \n",
      "2  {'nominees': [{'firstName': 'Brandy', 'lastNam...   \n",
      "3  {'nominees': [{'firstName': 'Jeffrey', 'lastNa...   \n",
      "4  {'nominees': [{'firstName': 'Karoline', 'lastN...   \n",
      "\n",
      "                                             request retrieval_date  \n",
      "0  {'url': 'https://api.congress.gov/v3/nominatio...     2025-07-13  \n",
      "1  {'url': 'https://api.congress.gov/v3/nominatio...     2025-07-13  \n",
      "2  {'url': 'https://api.congress.gov/v3/nominatio...     2025-07-13  \n",
      "3  {'url': 'https://api.congress.gov/v3/nominatio...     2025-07-13  \n",
      "4  {'url': 'https://api.congress.gov/v3/nominatio...     2025-07-13  \n"
     ]
    }
   ],
   "source": [
    "# Preview the nominees\n",
    "print(nominees_df.head())\n",
    "all_dataframes['nominees'] = nominees_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: determine whether safe to move this to other notebook, or if other code already depends on it happening this early\n",
    "## Normalize column names, leaving data values as-is\n",
    "#nominees_df.columns = [col.casefold().replace(' ', '_') for col in nominees_df.columns]\n",
    "#print(\"\\nNominees DataFrame columns:\")\n",
    "#for col in sorted(nominees_df.columns):\n",
    "#     print(f\"- {col}: {nominees_df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confirm \"nid\" and \"citation\" uniqueness to later use as FJC and Congress indexes, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'judges'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[1mAll nid values are unique\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'demographics'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[1mAll nid values are unique\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'education'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m3350 duplicate nid values found\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1386811: appears 5 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1382346: appears 5 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1385901: appears 5 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1377081: appears 5 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1386451: appears 5 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  ... and 3345 more duplicate values\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'federal_judicial_service'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m576 duplicate nid values found\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1390296: appears 6 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1381666: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1378736: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1385401: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1385551: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  ... and 571 more duplicate values\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'other_nominations_recess'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m92 duplicate nid values found\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1378126: appears 7 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1391841: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1386321: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1392521: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1382066: appears 3 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  ... and 87 more duplicate values\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'other_federal_judicial_service'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m62 duplicate nid values found\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1377121: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1392091: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1379716: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1378016: appears 3 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1393651: appears 3 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  ... and 57 more duplicate values\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'professional_career'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m3852 duplicate nid values found\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  13761909: appears 23 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1389246: appears 18 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  13761366: appears 17 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1380021: appears 16 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1380601: appears 16 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  ... and 3847 more duplicate values\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[1mChecking 'nid' uniqueness for dataframe 'seat_timeline'\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m576 duplicate nid values found\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1390296: appears 6 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1384186: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1385551: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1378736: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  1382066: appears 4 times\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mcheck_id_uniqueness\u001b[0m - \u001b[33m\u001b[1m  ... and 571 more duplicate values\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[33m\u001b[1mDataFrame '{name}' contains neither 'nid' nor 'citation' columns. Cannot determine source type.\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[33m\u001b[1mWARNING\u001b[0m | \u001b[36mvalidate_dataframe_ids\u001b[0m - \u001b[33m\u001b[1mDataFrame '{name}' contains neither 'nid' nor 'citation' columns. Cannot determine source type.\u001b[0m\n",
      "\u001b[32m2025-07-13 10:35:40\u001b[0m | \u001b[1mINFO\u001b[0m | \u001b[36m<module>\u001b[0m - \u001b[1m✓ All ID fields are unique across unique-ID-required dataframes.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ID uniqueness in dataframes before saving...\n"
     ]
    }
   ],
   "source": [
    "# Check for uniqueness in ID fields before saving to the raw data folder\n",
    "from nomination_predictor.dataset import validate_dataframe_ids\n",
    "\n",
    "print(\"Checking ID uniqueness in dataframes before saving...\")\n",
    "\n",
    "uniqueness_results = validate_dataframe_ids(all_dataframes) # discovered not all dataframes treat nid as unique due to re-appointments, position changes, etc.\n",
    "\n",
    "# Check if any dataframes have duplicate IDs\n",
    "problematic_dfs = [name for name, result in uniqueness_results.items() \n",
    "                   if not result.get('is_unique', True)]\n",
    "\n",
    "# if you want an easily-intuitive reason why a dataframe may not be able to use nid uniquely, try adding \"education\" to the \"uniqueness required\" list iterated through below, and see what it outputs.\n",
    "# you'll find numerous judges who are listed multiple times for having gotten different college or university degrees over the years.\n",
    "if any(name in [\"judges\", \"demographics\", \"nominations\", \"nominees\",] for name in problematic_dfs):\n",
    "    logger.warning(f\"⚠️ Found non-unique IDs in: {', '.join(problematic_dfs)}\")\n",
    "    for df_name in problematic_dfs:\n",
    "        result = uniqueness_results[df_name]\n",
    "        logger.warning(f\"\\nDuplicates in {df_name}:\")\n",
    "        display(result['duplicate_rows'])\n",
    "else:\n",
    "    \n",
    "    logger.info(\"✓ All ID fields are unique across unique-ID-required dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Data to Raw Directory\n",
    "\n",
    "Save the datasets to the raw data directory for use by downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataframes to /home/wsl2ubuntuuser/nomination_predictor/data/raw...\n",
      "  ✓ Saved 4,022 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/judges.csv\n",
      "  ✓ Saved 4,022 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/demographics.csv\n",
      "  ✓ Saved 8,040 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/education.csv\n",
      "  ✓ Saved 4,720 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/federal_judicial_service.csv\n",
      "  ✓ Saved 828 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/other_nominations_recess.csv\n",
      "  ✓ Saved 611 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/other_federal_judicial_service.csv\n",
      "  ✓ Saved 19,003 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/professional_career.csv\n",
      "  ✓ Saved 4,720 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/seat_timeline.csv\n",
      "  ✓ Saved 5,746 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/nominations.csv\n",
      "  ✓ Saved 5,671 records to /home/wsl2ubuntuuser/nomination_predictor/data/raw/nominees.csv\n",
      "\n",
      "✅ Successfully saved 10 dataframes to /home/wsl2ubuntuuser/nomination_predictor/data/raw\n",
      "✓ Saved 10 files to /home/wsl2ubuntuuser/nomination_predictor/data/raw\n",
      "✓ Created manifest: fjc_data_manifest_20250713.txt\n"
     ]
    }
   ],
   "source": [
    "# Save data to the raw data directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from nomination_predictor.config import RAW_DATA_DIR\n",
    "\n",
    "# Create the raw data directory if it doesn't exist\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Add a timestamp for the manifest\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Save each FJC dataframe\n",
    "# Save all dataframes to the raw data directory\n",
    "print(f\"Saving dataframes to {RAW_DATA_DIR}...\")\n",
    "saved_files = []\n",
    "\n",
    "# Ensure the output directory exists\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save all dataframes from the all_dataframes collection\n",
    "for name, df in all_dataframes.items():\n",
    "    if df is not None and not df.empty:\n",
    "        try:\n",
    "            # Create filename\n",
    "            output_file = RAW_DATA_DIR / f\"{name}.csv\"\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(output_file, index=False)\n",
    "            saved_files.append(f\"{name}.csv\")\n",
    "            print(f\"  ✓ Saved {len(df):,} records to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error saving {name}: {str(e)}\")\n",
    "\n",
    "# Print summary\n",
    "if saved_files:\n",
    "    print(f\"\\n✅ Successfully saved {len(saved_files)} dataframes to {RAW_DATA_DIR}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No dataframes were saved - check if all_dataframes is populated correctly\")\n",
    "\n",
    "# Create a manifest file to track what was saved and when\n",
    "manifest_content = f\"\"\"# FJC Data Processing Manifest\n",
    "Processed on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "Note: Only column names are normalized (lowercase with underscores), data values remain unchanged\n",
    "Files saved:\n",
    "{chr(10).join(['- ' + file for file in saved_files])}\n",
    "\"\"\"\n",
    "\n",
    "with open(RAW_DATA_DIR / f\"fjc_data_manifest_{timestamp}.txt\", \"w\") as f:\n",
    "    f.write(manifest_content)\n",
    "\n",
    "print(f\"✓ Saved {len(saved_files)} files to {RAW_DATA_DIR}\")\n",
    "print(f\"✓ Created manifest: fjc_data_manifest_{timestamp}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving nominations to cache file: /home/wsl2ubuntuuser/nomination_predictor/data/raw/nominations.csv\n",
      "✓ Saved 5746 nominations to cache\n",
      "Saving nominees to cache file: /home/wsl2ubuntuuser/nomination_predictor/data/raw/nominees.csv\n",
      "✓ Saved 5671 nominees to cache\n"
     ]
    }
   ],
   "source": [
    "# Save Congress API retrieved nominations to cache file\n",
    "if nominations_df is not None and not nominations_df.empty:\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(nominations_cache_file), exist_ok=True)\n",
    "    print(f\"Saving nominations to cache file: {nominations_cache_file}\")\n",
    "    nominations_df.to_csv(nominations_cache_file, index=False)\n",
    "    print(f\"✓ Saved {len(nominations_df)} nominations to cache\")\n",
    "    \n",
    "if nominees_df is not None and not nominees_df.empty:\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(nominees_cache_file), exist_ok=True)\n",
    "    print(f\"Saving nominees to cache file: {nominees_cache_file}\")\n",
    "    nominees_df.to_csv(nominees_cache_file, index=False)\n",
    "    print(f\"✓ Saved {len(nominees_df)} nominees to cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded Federal Judicial Center (FJC) data, the canonical source for judicial seats and judges\n",
    "2. Built a raw seat timeline dataframe inferred from the FJC service data\n",
    "3. Fetched judicial nominations from the Congress.gov API\n",
    "4. Fetched judicial nominee data from the Congress.gov API\n",
    "5. Saved all datasets to the raw data directory for further processing by downstream notebooks\n",
    "\n",
    "The next notebook (e.g. 1.##-nw-feature-engineering.ipynb) will load these datasets, clean them, and engineer features for modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomination_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
