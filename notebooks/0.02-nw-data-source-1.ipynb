{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ba9b4bd",
      "metadata": {},
      "source": [
        "# Judicial Vacancies Data Source Exploration\n",
        "\n",
        "This notebook demonstrates how to use the `make_dataset` module to fetch and process judicial vacancy data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "We'll:\n",
        "1. Import the necessary modules\n",
        "2. Fetch HTML data from the judicial vacancies archive\n",
        "3. Extract vacancy data from the HTML\n",
        "4. Convert the data to a pandas DataFrame\n",
        "5. Save the raw data to a CSV file\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9d4eafd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nomination_predictor      0.0.1             /home/wsl2ubuntuuser/nomination_predictor\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip list | grep nomination_predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6398e94a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'make_dataset' from 'nomination_predictor' (/home/wsl2ubuntuuser/nomination_predictor/nomination_predictor/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Import our data processing module\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnomination_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_dataset\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Set up paths\u001b[39;00m\n\u001b[32m     14\u001b[39m PROJECT_ROOT = Path().resolve().parent.parent\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'make_dataset' from 'nomination_predictor' (/home/wsl2ubuntuuser/nomination_predictor/nomination_predictor/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Enable autoreload for development\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Import standard libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Import our data processing module\n",
        "from nomination_predictor import make_dataset\n",
        "\n",
        "# Set up paths\n",
        "PROJECT_ROOT = Path().resolve().parent.parent\n",
        "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
        "DATA_RAW.mkdir(parents=True, exist_ok=True)  # Ensure directory exists"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0a841d",
      "metadata": {},
      "source": [
        "## 1. Fetch and Process Data\n",
        "\n",
        "Let's fetch the data for a specific year and process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208432ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_and_process_year(year=2023):\n",
        "    \"\"\"Fetch and process data for a specific year.\"\"\"\n",
        "    print(f'Fetching data for {year}...')\n",
        "    \n",
        "    # Generate or fetch archive URLs for the year\n",
        "    urls = make_dataset.generate_or_fetch_archive_urls(year)\n",
        "    \n",
        "    if not urls:\n",
        "        print(f'No URLs found for {year}')\n",
        "        return None\n",
        "    \n",
        "    print(f'Found {len(urls)} URLs for {year}')\n",
        "    \n",
        "    # Process each URL\n",
        "    all_records = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            # Fetch HTML content\n",
        "            html_content = make_dataset.fetch_html(url)\n",
        "            \n",
        "            # Extract records from HTML\n",
        "            records = make_dataset.extract_vacancy_table(html_content)\n",
        "            all_records.extend(records)\n",
        "            print(f' - Extracted {len(records)} records from {url}')\n",
        "        except Exception as e:\n",
        "            print(f'Error processing {url}: {e}')\n",
        "    \n",
        "    if not all_records:\n",
        "        print('No records found.')\n",
        "        return None\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = make_dataset.records_to_dataframe(all_records)\n",
        "    \n",
        "    # Add year column if not present\n",
        "    if 'year' not in df.columns:\n",
        "        df['year'] = year\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01164b16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Fetch data for 2023\n",
        "year_to_fetch = 2023\n",
        "df = fetch_and_process_year(year_to_fetch)\n",
        "\n",
        "if df is not None:\n",
        "    print(f'\\nFirst few records for {year_to_fetch}:')\n",
        "    display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390ee9ae",
      "metadata": {},
      "source": [
        "## 2. Save Raw Data\n",
        "\n",
        "Save the raw data to a CSV file in the `data/raw` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba05821a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_raw_data(df, year):\n",
        "    \"\"\"Save the raw data to a CSV file.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print('No data to save.')\n",
        "        return\n",
        "    \n",
        "    filename = DATA_RAW / f'judicial_vacancies_{year}.csv'\n",
        "    try:\n",
        "        make_dataset.save_to_csv(df, filename)\n",
        "        print(f'Data saved to {filename}')\n",
        "    except Exception as e:\n",
        "        print(f'Error saving data: {e}')\n",
        "\n",
        "# Save the data we just fetched\n",
        "if df is not None:\n",
        "    save_raw_data(df, year_to_fetch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3f6baa",
      "metadata": {},
      "source": [
        "## 3. Load and Explore the Saved Data\n",
        "\n",
        "Let's verify that we can load the saved data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf803db",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_raw_data(year):\n",
        "    \"\"\"Load raw data from a CSV file.\"\"\"\n",
        "    filename = DATA_RAW / f'judicial_vacancies_{year}.csv'\n",
        "    if not filename.exists():\n",
        "        print(f'File not found: {filename}')\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(filename)\n",
        "        print(f'Loaded {len(df)} records from {filename}')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f'Error loading {filename}: {e}')\n",
        "        return None\n",
        "\n",
        "# Load the data we just saved\n",
        "loaded_df = load_raw_data(year_to_fetch)\n",
        "if loaded_df is not None:\n",
        "    print('\\nDataFrame info:')\n",
        "    display(loaded_df.info())\n",
        "    print('\\nFirst few records:')\n",
        "    display(loaded_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6720fbb0",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Data Cleaning**: In the next notebook, we'll clean and preprocess this data.\n",
        "2. **Exploratory Analysis**: We'll explore the data to understand its structure and quality.\n",
        "3. **Feature Engineering**: We'll create additional features that might be useful for analysis.\n",
        "4. **Visualization**: We'll create visualizations to understand trends and patterns."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nomination_predictor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
