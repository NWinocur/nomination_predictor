{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ba9b4bd",
      "metadata": {},
      "source": [
        "# Judicial Vacancies Data Source Exploration\n",
        "\n",
        "This notebook demonstrates how to use the `dataset` module to fetch and process judicial vacancy data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "We'll:\n",
        "1. Fetch and process judicial vacancy data using the dataset module\n",
        "2. Save the processed data for further analysis\n",
        "3. Load the data and perform exploratory data analysis (e.g. visualizations)\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4eafd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip list | grep nomination_predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6398e94a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import our data processing module\n",
        "from nomination_predictor import dataset\n",
        "from nomination_predictor.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
        "\n",
        "# Set up visualization style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d4e0182",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the data directory exists and is writable\n",
        "if not RAW_DATA_DIR.exists():\n",
        "    print(f\"Error: Data directory does not exist: {RAW_DATA_DIR}\")\n",
        "elif not os.access(RAW_DATA_DIR, os.W_OK):\n",
        "    print(f\"Error: No write permission for directory: {RAW_DATA_DIR}\")\n",
        "else:\n",
        "    print(f\"Data directory is ready: {RAW_DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0a841d",
      "metadata": {},
      "source": [
        "## 1. Fetch and Process Data\n",
        "\n",
        "Let's fetch the data for the range of available years and process it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1116a50d",
      "metadata": {},
      "outputs": [],
      "source": [
        "YEARS_BACK = 16\n",
        "raw_combined_csv_path = RAW_DATA_DIR / \"judicial_data.csv\"\n",
        "raw_vacancies_csv_path = RAW_DATA_DIR / \"judicial_vacancies.csv\"\n",
        "raw_confirmations_csv_path = RAW_DATA_DIR / \"judicial_confirmations.csv\"\n",
        "raw_emergencies_csv_path = RAW_DATA_DIR / \"judicial_emergencies.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208432ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_judicial_data(\n",
        "        years_back: int = YEARS_BACK,\n",
        "        force_refresh: bool = False,\n",
        "        output_dir: Path = RAW_DATA_DIR,\n",
        "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Load judicial data from already-saved CSV file if it is found; if not found (or if refresh forced via input arg), uses dataset module to re-retrieve from Internet into new dataframes.\n",
        "        Does not attempt to save new CSV files with those dataframes; see save_judicial_data() for that.\n",
        "        \n",
        "        Args:\n",
        "            years_back: Number of years of historical data to fetch\n",
        "            force_refresh: If True, force refetching data even if files exist\n",
        "            output_dir: Directory to save/load data files\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of (vacancies_df, confirmations_df, emergencies_df, combined_df)\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Only run the pipeline if output doesn't exist or force_refresh is True\n",
        "            if force_refresh or not raw_combined_csv_path.exists():\n",
        "                print(\"Running data pipeline...\")\n",
        "                with tqdm(total=4, desc=\"Processing data\") as progress_bar:\n",
        "                    # Run the main data pipeline\n",
        "                    combined_df = dataset.main(\n",
        "                        output_dir=output_dir,\n",
        "                        output_filename=raw_combined_csv_path.name,\n",
        "                        years_back=years_back\n",
        "                    )\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "                    \n",
        "                    # Read the data with progress\n",
        "                    progress_bar.set_description(\"Loading individual datasets\")\n",
        "                    combined_df = pd.read_csv(raw_combined_csv_path, sep='|') if raw_combined_csv_path.exists() else pd.DataFrame()\n",
        "                    progress_bar.update(1)\n",
        "                    \n",
        "                    vacancies_df = pd.read_csv(raw_vacancies_csv_path, sep='|') if raw_vacancies_csv_path.exists() else pd.DataFrame()\n",
        "                    progress_bar.update(1)\n",
        "                    \n",
        "                    confirmations_df = pd.read_csv(raw_confirmations_csv_path, sep='|') if raw_confirmations_csv_path.exists() else pd.DataFrame()\n",
        "                    emergencies_df = pd.read_csv(raw_emergencies_csv_path, sep='|') if raw_emergencies_csv_path.exists() else pd.DataFrame()\n",
        "                    progress_bar.update(1)\n",
        "                    \n",
        "            else:\n",
        "                print(\"Loading cached data...\")\n",
        "                combined_df = pd.read_csv(raw_combined_csv_path, sep='|') if raw_combined_csv_path.exists() else pd.DataFrame()\n",
        "                vacancies_df = pd.read_csv(raw_vacancies_csv_path, sep='|') if raw_vacancies_csv_path.exists() else pd.DataFrame()\n",
        "                confirmations_df = pd.read_csv(raw_confirmations_csv_path, sep='|') if raw_confirmations_csv_path.exists() else pd.DataFrame()\n",
        "                emergencies_df = pd.read_csv(raw_emergencies_csv_path, sep='|') if raw_emergencies_csv_path.exists() else pd.DataFrame()\n",
        "                \n",
        "            return vacancies_df, confirmations_df, emergencies_df, combined_df\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbdd687",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data_file = Path(raw_combined_csv_path)\n",
        "data_refreshed: bool = False\n",
        "\n",
        "# Check if file exists and is not empty\n",
        "if data_file.exists() and os.path.getsize(data_file) > 0:\n",
        "    print(\"Loading existing judicial data...\")\n",
        "    vacancies_df, confirmations_df, emergencies_df, combined_df = load_judicial_data(\n",
        "        years_back=YEARS_BACK,\n",
        "        force_refresh=False  # Use existing data\n",
        "    )\n",
        "else:\n",
        "    print(\"No existing data found or file is empty. Fetching fresh data...\")\n",
        "    vacancies_df, confirmations_df, emergencies_df, combined_df = load_judicial_data(\n",
        "        years_back=YEARS_BACK,\n",
        "        force_refresh=True  # Force fetch new data\n",
        "    )\n",
        "    data_refreshed = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390ee9ae",
      "metadata": {},
      "source": [
        "## 2. Save raw Data\n",
        "\n",
        "Save the raw data to a CSV file in the `data/raw` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b28d127",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save dataframes to csv files if we got new data\n",
        "from nomination_predictor.dataset import save_dataframe_to_csv\n",
        "\n",
        "if data_refreshed and (combined_df is not None) and not combined_df.empty:\n",
        "    save_dataframe_to_csv(combined_df, \"judicial_data\", RAW_DATA_DIR)\n",
        "        \n",
        "    # Save individual processed datasets\n",
        "    if not vacancies_df.empty:\n",
        "        save_dataframe_to_csv(vacancies_df, \"judicial_vacancies\", RAW_DATA_DIR)\n",
        "    if not confirmations_df.empty:\n",
        "        save_dataframe_to_csv(confirmations_df, \"judicial_confirmations\", RAW_DATA_DIR)\n",
        "    if not emergencies_df.empty:\n",
        "        save_dataframe_to_csv(emergencies_df, \"judicial_emergencies\", RAW_DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa3f6baa",
      "metadata": {},
      "source": [
        "## 3. Load either Saved or Internet-Retrieved Data\n",
        "\n",
        "Let's verify that we can load the saved data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b142ea39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\\n\",\n",
        "print(\"Loading judicial data...\")\n",
        "vacancies_df, confirmations_df, emergencies_df, combined_df = load_judicial_data(\n",
        "    years_back=YEARS_BACK,\n",
        "    force_refresh=False  # Set to True to refetch data\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd25290b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display data summary\n",
        "def display_data_summary():\n",
        "    \"\"\"Display summary of loaded data.\"\"\"\n",
        "    print(\"\\nData Summary:\")\n",
        "    print(f\"Vacancies: {len(vacancies_df)} records\")\n",
        "    print(f\"Confirmations: {len(confirmations_df)} records\")\n",
        "    print(f\"Emergencies: {len(emergencies_df)} records\")\n",
        "    print(f\"Combined: {len(combined_df)} total records\")\n",
        "    \n",
        "    if not combined_df.empty:\n",
        "        print(\"\\nDate ranges:\")\n",
        "        for col in ['vacancy_date', 'nomination_date', 'confirmation_date']:\n",
        "            if col in combined_df.columns:\n",
        "                dates = pd.to_datetime(combined_df[col], errors='coerce')\n",
        "                valid_dates = dates[dates.notna()]\n",
        "                if not valid_dates.empty:\n",
        "                    print(f\"{col}: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
        "\n",
        "display_data_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ff5713",
      "metadata": {},
      "outputs": [],
      "source": [
        "# trim & normalize string column names (leave upper/lower casing of string values as-is; only modifying column titles)\n",
        "for df in (vacancies_df, confirmations_df, emergencies_df):\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    str_cols = df.select_dtypes(include=['object']).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a520e87b",
      "metadata": {},
      "source": [
        "### Coverting dates to datetime format\n",
        "\n",
        "It'll be easier to perform data analysis if we we'll have to convert dates from string to datetime format. in order for that to work, we'll need to clean up one item clearly-incorrect in the source data: despite the Vacancy Confirmations report from 2015 January showing Claudia Ann Wilken assumed senior status in January 1st of the year 3000, according to https://www.fjc.gov/node/1389756 it really happened on December 17th, 2014 instead.  We'll correct for that in order for pd.to_datetime() to not choke on a far-future year..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9553aa7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "confirmations_df.loc[confirmations_df['incumbent'] == 'Wilken,Claudia', 'vacancy_date'] = '12/17/2014'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31945eb",
      "metadata": {},
      "source": [
        "Parse dates to convert any yyyy/mm/dd, mm/dd/yyyy, or empty string to datetime64[ns]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ca81a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "for df in (vacancies_df, confirmations_df, emergencies_df):\n",
        "    if df is not None and not df.empty:\n",
        "        df['vacancy_date'] = pd.to_datetime(df['vacancy_date'], errors='coerce')\n",
        "\n",
        "if vacancies_df is not None and not vacancies_df.empty:\n",
        "    vacancies_df['nomination_date'] = pd.to_datetime(vacancies_df['nomination_date'], errors='coerce')\n",
        "        \n",
        "if confirmations_df is not None and not confirmations_df.empty: \n",
        "    confirmations_df['confirmation_date'] = pd.to_datetime(confirmations_df['confirmation_date'], errors='coerce')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f65d8b1",
      "metadata": {},
      "source": [
        "Split the “circuit-court” code into two columns (circuit, district_or_state) so “02 – NYS” and “02 – NYW” group sensibly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9263b430",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nomination_predictor.dataset import parse_circuit_court\n",
        "\n",
        "\n",
        "# Function to safely parse circuit and court\n",
        "def add_circuit_court_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add 'circuit' and 'court_code' columns to the dataframe by parsing the source column.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        source_col: Name of the column containing the court string to parse\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with added 'circuit' and 'court_code' columns\n",
        "    \"\"\"\n",
        "    # Initialize new columns\n",
        "    df['circuit'] = None\n",
        "    df['court_code'] = None\n",
        "    \n",
        "    # Parse each court string and fill the new columns\n",
        "    for idx, court_str in df['circuit_district'].items():\n",
        "        try:\n",
        "            circuit, court_code = parse_circuit_court(str(court_str))\n",
        "            df.at[idx, 'circuit'] = circuit\n",
        "            df.at[idx, 'court_code'] = court_code\n",
        "        except ValueError as e:\n",
        "            print(f\"Warning: Could not parse court string '{court_str}': {e}\")\n",
        "            df.at[idx, 'circuit'] = None\n",
        "            df.at[idx, 'court_code'] = court_str  # Keep original as fallback\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9574f24",
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename the differently-named, same-purpose column from vacancies tables to make looping easier:\n",
        "if not vacancies_df.empty and 'court' in vacancies_df.columns:\n",
        "    vacancies_df = vacancies_df.rename(columns={'court': 'circuit_district'})\n",
        "\n",
        "for df_name, df in [('vacancies_df', vacancies_df), ('confirmations_df', confirmations_df), ('emergencies_df', emergencies_df)]:\n",
        "    if not df.empty:\n",
        "        print(f\"\\nAdding circuit/court columns to {df_name}\")\n",
        "        df = add_circuit_court_columns(df)\n",
        "        display(df[['circuit_district', 'circuit', 'court_code']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9055f545",
      "metadata": {},
      "source": [
        "## Data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5b9d98",
      "metadata": {},
      "source": [
        "Let's start with any hand-checked corrections to incorrectly-written reports.  (Similar to what had to be done to the record about the year 3000 above, which was a prerequisite to us cleaning up datetime formats.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe221d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Delete a record for which the subsequent month's report corrected its info.  Incorrect record is easy to identify by its unexpected parenthetical numbers in a name field.\n",
        "# Subsequent month's report's info has been verified by comparing it with https://www.ca2.uscourts.gov/judges/bios/gel.html and https://www.senate.gov/legislative/LIS/roll_call_votes/vote1111/vote_111_1_00288.htm\n",
        "confirmations_df = confirmations_df.drop(confirmations_df[confirmations_df['nominee'] == \"Lynch,Gerard E. (103419)\"].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e1ef57",
      "metadata": {},
      "source": [
        "### Creating a *seat-vacancy key* for ourselves\n",
        "\n",
        "A vacancy can be uniquely defined by a consistent combination of circuit, court district, incumbent (or in the case of newly-opened roles, the lack of an incumbent), and vacancy date.  Over time it typically gets a nominee, then gets a confirmation. But those are a matter of its status or progress over time; the vacancy ID we'd give it would stay the same across months to indicate it's the same vacancy.\n",
        "\n",
        "The seat_id can end up looking something like:\n",
        "`seat_id:str = (circuit|court_code|incumbent|vacancy_date).casefold()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d00cd7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# after loading the emergencies dataframe, standardize the column name to be same as the way the other dataframes use that info\n",
        "if not emergencies_df.empty and 'vacancy_created_by' in emergencies_df.columns:\n",
        "    emergencies_df = emergencies_df.rename(columns={'vacancy_created_by': 'incumbent'})\n",
        "\n",
        "# standardizing incumbent name column makes \n",
        "def make_key(df) -> str:\n",
        "    # Use the new circuit and court_code columns instead of circuit_district because we just engineered the former pair to be more consistent across dataframes\n",
        "    circuit_info = df['circuit'].astype(str) + '-' + df['court_code'].fillna('')\n",
        "    return (\n",
        "        circuit_info\n",
        "        + \"|\" + df['incumbent'].fillna(\"POSITION OPEN\")\n",
        "        + \"|\" + df['vacancy_date'].dt.strftime(\"%Y-%m-%d\")\n",
        "    ).str.casefold()\n",
        "\n",
        "vacancies_df['seat_id'] = make_key(vacancies_df)\n",
        "vacancies_df['seat_id']  = make_key(vacancies_df)\n",
        "emergencies_df['seat_id'] = make_key(emergencies_df)\n",
        "confirmations_df['seat_id'] = make_key(confirmations_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6ee699",
      "metadata": {},
      "source": [
        "This lets us de-duplicate monthly re-listings, keeping the most-recent because it's more likely to contain a nominee instead of leave that field blank & to-be-determined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9963953",
      "metadata": {},
      "outputs": [],
      "source": [
        "def keep_latest_with_index(df, name=\"dataset\"):\n",
        "    \"\"\"Keep only the most recent entry for each seat_id and set it as index.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame with source_year, source_month, and seat_id columns\n",
        "        name: Name of the dataset for logging purposes\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with seat_id as index and most recent entries\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(f\"Skipping {name} - not found or empty\")\n",
        "        return None\n",
        "    \n",
        "    # Store original count for later reporting of how many dupes we dropped\n",
        "    original_count = len(df)\n",
        "    \n",
        "    # Create temporary date column safely\n",
        "    df = df.copy()\n",
        "    df['last_report_date'] = pd.to_datetime({\n",
        "        'year':  df['source_year'],\n",
        "        'month': df['source_month'],\n",
        "        'day':   1 # for our purposes we know we only get 1 report per type per month, so day=1 is a good-enough simplification even though technically false about our end-of-month reports\n",
        "    })\n",
        "    \n",
        "    # Get most recent entry per seat_id and set index\n",
        "    latest = (df\n",
        "             .sort_values(['seat_id', 'last_report_date'], ascending=[True, False])\n",
        "             .drop_duplicates('seat_id', keep='first')\n",
        "             .set_index('seat_id', drop=False))  # Keep seat_id as both index and column\n",
        "    \n",
        "    # Report results\n",
        "    print(f\"{name}: {len(latest)} unique seats (removed {original_count - len(latest)} duplicates)\")\n",
        "    return latest\n",
        "\n",
        "# Process dataframes with seat_id as index\n",
        "dfs = {\n",
        "    'vacancies': keep_latest_with_index(vacancies_df, \"Vacancies\"),\n",
        "    'confirmations': keep_latest_with_index(confirmations_df, \"Confirmations\"),\n",
        "    'emergencies': keep_latest_with_index(emergencies_df, \"Emergencies\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a52f11",
      "metadata": {},
      "source": [
        "## Seat-Id-aware combining into merged dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70cac04f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for df in [vacancies_df, confirmations_df, emergencies_df]:\n",
        "    print(df.columns) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2b31ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "def combine_dataframes(confirmations_df, emergencies_df, vacancies_df):\n",
        "    \"\"\"Combine dataframes with confirmations having highest priority, then emergencies, then vacancies.\n",
        "    Only non-null values will overwrite existing values.\n",
        "    \"\"\"\n",
        "    # Start with the lowest priority (vacancies)\n",
        "    combined = vacancies_df.copy()\n",
        "    \n",
        "    # Function to safely update a series with non-null values\n",
        "    def safe_update(original, new):\n",
        "        if pd.isna(new) or new is None:\n",
        "            return original\n",
        "        return new\n",
        "    \n",
        "    # Process emergencies first (medium priority)\n",
        "    if not emergencies_df.empty:\n",
        "        emergencies_dict = emergencies_df.set_index('seat_id').to_dict('index')\n",
        "        for seat_id, row in emergencies_dict.items():\n",
        "            if seat_id in combined['seat_id'].values:\n",
        "                # Update only non-null values\n",
        "                for col, value in row.items():\n",
        "                    if col in combined.columns and not pd.isna(value) and value is not None:\n",
        "                        combined.loc[combined['seat_id'] == seat_id, col] = value\n",
        "            else:\n",
        "                # Add new row if seat_id doesn't exist\n",
        "                combined = pd.concat([combined, pd.DataFrame([row])], ignore_index=True)\n",
        "    \n",
        "    # Then process confirmations (highest priority)\n",
        "    if not confirmations_df.empty:\n",
        "        confirmations_dict = confirmations_df.set_index('seat_id').to_dict('index')\n",
        "        for seat_id, row in confirmations_dict.items():\n",
        "            if seat_id in combined['seat_id'].values:\n",
        "                # Update only non-null values\n",
        "                for col, value in row.items():\n",
        "                    if col in combined.columns and not pd.isna(value) and value is not None:\n",
        "                        combined.loc[combined['seat_id'] == seat_id, col] = value\n",
        "            else:\n",
        "                # Add new row if seat_id doesn't exist\n",
        "                combined = pd.concat([combined, pd.DataFrame([row])], ignore_index=True)\n",
        "    \n",
        "    return combined\n",
        "\n",
        "# Usage:\n",
        "combined_df = combine_dataframes(\n",
        "    confirmations_df=dfs['confirmations'],\n",
        "    emergencies_df=dfs['emergencies'],\n",
        "    vacancies_df=dfs['vacancies']\n",
        ")\n",
        "\n",
        "# Drop unwanted columns\n",
        "columns_to_drop = ['circuit_district', 'source_year', 'source_month', 'source_page_type']\n",
        "combined_df = combined_df.drop(\n",
        "    columns=[col for col in columns_to_drop if col in combined_df.columns]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7cfe3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "if combined_df is not None:\n",
        "    print(\"\\nSample data from combined dataset:\")\n",
        "    display(combined_df.head())\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    \n",
        "    if 'vacancy_date' in combined_df.columns:\n",
        "        print(\"\\nDate Range:\")\n",
        "        # Convert from string to datetime so that comparisons actually compare\n",
        "        combined_df['vacancy_date_dt'] = pd.to_datetime(combined_df['vacancy_date'], format='%m/%d/%Y')\n",
        "        print(f\"Earliest vacancy: {combined_df['vacancy_date_dt'].min().strftime('%m/%d/%Y')}\")\n",
        "        print(f\"Latest vacancy: {combined_df['vacancy_date_dt'].max().strftime('%m/%d/%Y')}\")\n",
        "    \n",
        "    if 'circuit_district' in combined_df.columns:\n",
        "        print(\"\\nRecords by Circuit/District:\")\n",
        "        print(combined_df['circuit_district'].value_counts().head(10))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca3142d",
      "metadata": {},
      "source": [
        "# Features Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb7a0b3",
      "metadata": {},
      "source": [
        "Add fields which will become our regression targets.\n",
        "\n",
        "TODO: supplement these with additional \"elapsing only while the legisladtive branch was in session\" durations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18460f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(combined_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc35e38",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def safe_date_diff(end_date, start_date):\n",
        "    \"\"\"Calculate days between dates, returning None if either date is missing\"\"\"\n",
        "    if pd.isna(end_date) or pd.isna(start_date):\n",
        "        return np.nan\n",
        "    try:\n",
        "        return (end_date - start_date).days\n",
        "    except (TypeError, ValueError):\n",
        "        return np.nan\n",
        "\n",
        "# Calculate day differences safely\n",
        "combined_df['days_vac_to_nom'] = combined_df.apply(\n",
        "    lambda x: safe_date_diff(x['nomination_date'], x['vacancy_date']), \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "combined_df['days_nom_to_conf'] = combined_df.apply(\n",
        "    lambda x: safe_date_diff(x['confirmation_date'], x['nomination_date']), \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "combined_df['days_vac_to_conf'] = combined_df.apply(\n",
        "    lambda x: safe_date_diff(x['confirmation_date'], x['vacancy_date']), \n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479d7e0b",
      "metadata": {},
      "source": [
        "Add fields to indicate who was the sitting president on key dates, which presidential term and # of days into that presidential term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f242e10",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement test cases and function in features.py such that it can take a datetime-type input, and returns an int indicating which president was sitting in office at the time\n",
        "# TODO: call that function from here to add fields for each row indicating who was president at times of vacancy, nomination, and confirmation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957d7411",
      "metadata": {},
      "source": [
        "Add field to indicate which legislative session key dates occurred in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdfa3a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement test cases and function in features.py such that it can take a datetime-type input, and returns an int to indicate which legislative session it falls under.\n",
        "# TODO: decide how to handle in-between-sessions dates\n",
        "# TODO: call that function from here to add fields"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642f84d2",
      "metadata": {},
      "source": [
        "Add fields for how much government is/isn't unified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14cee7e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dc9698af",
      "metadata": {},
      "source": [
        "Add field to group by U.S. states because many have multiple regional districts (e.g. CA-N, CA-S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a0434f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "19a75c4e",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0651346a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def explore_data(df: pd.DataFrame, dataset_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate exploratory visualizations for a dataset.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to explore\n",
        "        dataset_name: Name of the dataset for titles\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        print(f\"No data available for {dataset_name}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nExploring {dataset_name} data:\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(\"\\nFirst few records:\")\n",
        "    display(df.head())\n",
        "    \n",
        "    # Plot time series if date columns exist\n",
        "    date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
        "    for date_col in date_columns:\n",
        "        if date_col in df.columns and not df[date_col].isna().all():\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "            time_series = df[date_col].dropna().value_counts().sort_index()\n",
        "            time_series.plot(kind='line', marker='o')\n",
        "            plt.title(f\"{dataset_name} by {date_col}\")\n",
        "            plt.xlabel(date_col)\n",
        "            plt.ylabel(\"Count\")\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "    \n",
        "    # Plot categorical data\n",
        "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() < 20]\n",
        "    for col in categorical_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        df[col].value_counts().sort_values(ascending=False).plot(kind='bar')\n",
        "        plt.title(f\"{col} Distribution\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "        time_series = df[date_col].dropna().value_counts().sort_index()\n",
        "        time_series.plot(kind='line', marker='o')\n",
        "        plt.title(f\"{dataset_name} by {date_col}\")\n",
        "        plt.xlabel(date_col)\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Plot categorical data\n",
        "    categorical_cols = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() < 20]\n",
        "    for col in categorical_cols:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        df[col].value_counts().sort_values(ascending=False).plot(kind='bar')\n",
        "        plt.title(f\"{col} Distribution\")\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Explore each dataset\n",
        "if not vacancies_df.empty:\n",
        "    explore_data(vacancies_df, \"Vacancies\")\n",
        "    \n",
        "if not confirmations_df.empty:\n",
        "    explore_data(confirmations_df, \"Confirmations\")\n",
        "    \n",
        "if not emergencies_df.empty:\n",
        "    explore_data(emergencies_df, \"Emergencies\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b5af1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not combined_df.empty:\n",
        "    print(\"\\nCombined Data Analysis:\")\n",
        "    \n",
        "    # Convert date columns\n",
        "    date_columns = [col for col in combined_df.columns if 'date' in col.lower()]\n",
        "    for col in date_columns:\n",
        "        combined_df[col] = pd.to_datetime(combined_df[col], errors='coerce')\n",
        "    \n",
        "    # Plot vacancy reasons\n",
        "    if 'vacancy_reason' in combined_df.columns:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        combined_df['vacancy_reason'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "        plt.title(\"Vacancy Reasons\")\n",
        "        plt.ylabel(\"\")\n",
        "        plt.show()\n",
        "    \n",
        "    # Plot vacancies over time\n",
        "    if 'vacancy_date' in combined_df.columns:\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        combined_df.set_index('vacancy_date').resample('M').size().plot()\n",
        "        plt.title(\"Monthly Vacancies Over Time\")\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Number of Vacancies\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6720fbb0",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Data Cleaning**: In the next notebook, we'll clean and preprocess this data.\n",
        "2. **Exploratory Analysis**: We'll explore the data to understand its structure and quality.\n",
        "3. **Feature Engineering**: We'll create additional features that might be useful for analysis.\n",
        "4. **Visualization**: We'll create visualizations to understand trends and patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5176af99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup and Imports\n",
        "# Set up logging\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "from nomination_predictor.congress_api import CongressAPIClient\n",
        "# Import the function from your module\n",
        "from nomination_predictor.dataset import compare_and_validate_api_data\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29b03bb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Fetch Data from Congress.gov API\n",
        "def fetch_api_data() -> pd.DataFrame:\n",
        "    \"\"\"Fetch data from Congress.gov API and return as a DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        congresses_back: Number of past congresses to include (default: 5)\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing judicial nomination data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = CongressAPIClient()\n",
        "        current_congress = 118  # TODO: expand to look through prior congresses after proof of concept with just one\n",
        "        logger.info(f\"Fetching data for congress {current_congress}...\")\n",
        "        \n",
        "        # Get list of nomination records\n",
        "        nominations = client.get_judicial_nominations(current_congress)\n",
        "        \n",
        "        # Convert list of dicts to DataFrame\n",
        "        if nominations:  # Only create DataFrame if we have data\n",
        "            return pd.DataFrame(nominations)\n",
        "        else:\n",
        "            logger.error(\"No data retrieved from API\")\n",
        "            return pd.DataFrame()  # Return empty DataFrame if no data\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching API data: {e}\")\n",
        "        return pd.DataFrame()  # Return empty DataFrame on error\n",
        "\n",
        "# Fetch the data\n",
        "api_df = fetch_api_data()\n",
        "print(api_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a000c0df",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 3: Run the Comparison\n",
        "def run_comparison_analysis(api_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Run the comparison and display results.\"\"\"\n",
        "    if api_df.empty:\n",
        "        return {\"error\": \"No API data available for comparison\"}\n",
        "    \n",
        "    # Run the comparison\n",
        "    results = compare_and_validate_api_data(api_df)\n",
        "    \n",
        "    # Display summary\n",
        "    display(Markdown(\"## 📊 Schema Comparison Summary\"))\n",
        "    display(pd.DataFrame({\n",
        "        \"Metric\": [\n",
        "            \"API Records\", \n",
        "            \"Legacy Vacancy Records\", \n",
        "            \"Legacy Confirmation Records\",\n",
        "            \"Vacancy Schema Coverage\",\n",
        "            \"Confirmation Schema Coverage\",\n",
        "            \"Overall Compatibility Score\"\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            results[\"api_record_count\"],\n",
        "            results.get(\"vacancy_record_count\", \"N/A\"),\n",
        "            results.get(\"confirmation_record_count\", \"N/A\"),\n",
        "            f\"{results['schema_comparison']['vacancy'].get('coverage_percentage', 0):.1f}%\",\n",
        "            f\"{results['schema_comparison']['confirmation'].get('coverage_percentage', 0):.1f}%\",\n",
        "            f\"{results.get('compatibility_score', 0):.1f}%\"\n",
        "        ]\n",
        "    }))\n",
        "    \n",
        "    # Display critical fields analysis\n",
        "    display(Markdown(\"## 🔍 Critical Fields Analysis\"))\n",
        "    critical_data = []\n",
        "    for source in [\"vacancy\", \"confirmation\"]:\n",
        "        if f\"missing_critical_{source}_fields\" in results:\n",
        "            for field in results[f\"missing_critical_{source}_fields\"] or []:\n",
        "                critical_data.append({\n",
        "                    \"Source\": source.capitalize(),\n",
        "                    \"Critical Field\": field,\n",
        "                    \"Status\": \"❌ Missing\"\n",
        "                })\n",
        "    \n",
        "    if critical_data:\n",
        "        display(pd.DataFrame(critical_data))\n",
        "    else:\n",
        "        display(Markdown(\"✅ All critical fields are present in the API data\"))\n",
        "    \n",
        "    # Data quality analysis\n",
        "    display(Markdown(\"## 📈 Data Quality Analysis\"))\n",
        "    if results.get(\"data_quality\"):\n",
        "        null_df = pd.DataFrame(\n",
        "            results[\"data_quality\"][\"null_percentage\"].items(),\n",
        "            columns=[\"Field\", \"Null Percentage\"]\n",
        "        ).sort_values(\"Null Percentage\", ascending=False)\n",
        "        \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        sns.barplot(data=null_df.head(10), x=\"Null Percentage\", y=\"Field\")\n",
        "        plt.title(\"Top 10 Fields by Null Percentage\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        if results[\"data_quality\"][\"high_null_fields\"]:\n",
        "            display(Markdown(f\"⚠️ High null percentage (>50%) in fields: {', '.join(results['data_quality']['high_null_fields'])}\"))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run the analysis\n",
        "results = run_comparison_analysis(api_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0fe552",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Cell 4: Detailed Field Comparison (Optional)\n",
        "def display_field_comparison(results: Dict[str, Any]) -> None:\n",
        "    \"\"\"Display detailed field comparison between schemas.\"\"\"\n",
        "    display(Markdown(\"## 🔄 Detailed Field Comparison\"))\n",
        "    \n",
        "    for source in [\"vacancy\", \"confirmation\"]:\n",
        "        if source in results[\"schema_comparison\"]:\n",
        "            comp = results[\"schema_comparison\"][source]\n",
        "            display(Markdown(f\"### {source.capitalize()} Schema\"))\n",
        "            \n",
        "            # Common fields\n",
        "            display(Markdown(\"#### ✅ Common Fields\"))\n",
        "            display(pd.DataFrame(comp[\"common_fields\"], columns=[\"Field\"]))\n",
        "            \n",
        "            # Missing fields\n",
        "            if comp[\"missing_from_api\"]:\n",
        "                display(Markdown(\"#### ❌ Fields Missing from API\"))\n",
        "                display(pd.DataFrame(comp[\"missing_from_api\"], columns=[\"Field\"]))\n",
        "            \n",
        "            # Extra fields\n",
        "            if comp[\"extra_in_api\"]:\n",
        "                display(Markdown(\"#### ➕ Extra Fields in API\"))\n",
        "                display(pd.DataFrame(comp[\"extra_in_api\"], columns=[\"Field\"]))\n",
        "\n",
        "# Uncomment to see detailed field comparison\n",
        "display_field_comparison(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nomination_predictor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
