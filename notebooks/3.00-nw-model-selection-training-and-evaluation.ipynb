{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b092c37",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280e74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-18 15:27:02.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mProject root: /home/wsl2ubuntuuser/nomination_predictor\u001b[0m\n",
      "\u001b[32m2025-07-18 15:27:02.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mConfiguration loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from nomination_predictor.config import MODELS_DIR, PROCESSED_DATA_DIR\n",
    "from nomination_predictor.modeling.predict import (compare_models,\n",
    "                                                   evaluate_and_report_model,\n",
    "                                                   train_and_evaluate_model)\n",
    "from nomination_predictor.modeling.train import save_model_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PROCESSED_DATA_DIR/\"processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52963229",
   "metadata": {},
   "source": [
    "# Verify GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf384ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ XGBoost version: 3.0.2\n",
      "âœ… GPU detected:\n",
      "  ðŸ”¹ NVIDIA GeForce RTX 3070, 8192 MiB\n",
      "  ðŸ”¹ NVIDIA GeForce RTX 2060, 6144 MiB\n",
      "âœ… XGBoost GPU support confirmed with current syntax!\n"
     ]
    }
   ],
   "source": [
    "# Verify current GPU setup\n",
    "import subprocess\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"ðŸ“¦ XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# Test GPU availability\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… GPU detected:\")\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            print(f\"  ðŸ”¹ {line}\")\n",
    "    else:\n",
    "        print(\"âŒ nvidia-smi failed\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ nvidia-smi not found\")\n",
    "\n",
    "# Test XGBoost GPU support\n",
    "try:\n",
    "    test_model = xgb.XGBRegressor(tree_method=\"hist\", device=\"cuda\")\n",
    "    print(\"âœ… XGBoost GPU support confirmed with current syntax!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ XGBoost GPU issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a664c",
   "metadata": {},
   "source": [
    "# Choose features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b090b",
   "metadata": {},
   "source": [
    "## target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4addbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"days_nom_to_conf\"\n",
    "\n",
    "# pick target and drop Y label targets from features\n",
    "y = df[TARGET]\n",
    "X = df.drop(columns=[TARGET, \"days_nom_to_latest_action\"])  # other target saved for later tries at modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2378c",
   "metadata": {},
   "source": [
    "## numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75663fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = [\n",
    "    \"actions_count\",\n",
    "    \"age_at_nom_days\",\n",
    "    \"birth_year\", \n",
    "    \"committees_count\",\n",
    "    \"congress_num\", \n",
    "    \"days_into_pres_term\",\n",
    "    \"days_nom_to_deceased\",\n",
    "    \"days_to_next_midterm_election\",\n",
    "    \"days_to_next_pres_election\",\n",
    "    \"death_year\", \n",
    "    \"degree_year\", \n",
    "    \"education_sequence\", \n",
    "    \"fed_service_sequence\", \n",
    "    \"highest_degree_level\",\n",
    "    \"professional_career_sequence\",\n",
    "    \"record_vote_number\",   \n",
    "    \"service_as_chief_judge,_begin\", \n",
    "    \"service_as_chief_judge,_end\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6168585",
   "metadata": {},
   "source": [
    "# boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_FEATURES = [\n",
    "    \"pres_term_is_latter_term\", \n",
    "    \"statute_authorized_new_seat_bool\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2bbf16",
   "metadata": {},
   "source": [
    "# categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69674cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES  = [\n",
    "    \"aba_rating\", \n",
    "    \"appointing_president\",\n",
    "    \"congress_session\",\n",
    "    \"court_type\",\n",
    "    \"birth_state\",\n",
    "    \"latestaction_is_div_opp_house\",\n",
    "    \"latestaction_is_div_opp_senate\",\n",
    "    \"latestaction_is_fully_div\",\n",
    "    \"latestaction_is_unified\",\n",
    "    \"nomination_vacancy_reason\",\n",
    "    \"nomination_of_or_from_location\",\n",
    "    \"nomination_to_position_title\",\n",
    "    \"nomination_to_court_name\",\n",
    "    \"nominees_0_organization\",\n",
    "    \"nominees_0_state\",\n",
    "    \"nomination_term_years\", # sounds numeric but only few options, including lifetime\n",
    "    \"party_of_appointing_president\",\n",
    "    \"race_or_ethnicity\",\n",
    "    \"received_in_senate_political_era\",\n",
    "    \"school\",\n",
    "    \"seat_level_cong_recategorized\",\n",
    "    \"seat_id_letters_only\",\n",
    "    \"senate_vote_type\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec7c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All features are unique across feature type lists\n",
      "â„¹ï¸ Total unique features: 43\n"
     ]
    }
   ],
   "source": [
    "from nomination_predictor.modeling.train import validate_feature_lists\n",
    "\n",
    "are_features_unique = validate_feature_lists(NUMERIC_FEATURES, BOOLEAN_FEATURES, CATEGORICAL_FEATURES)\n",
    "\n",
    "if not are_features_unique:\n",
    "    raise ValueError(\"Feature lists contain duplicates. Please fix before continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa37ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len([col for col in NUMERIC_FEATURES if col not in df.columns]) > 0:\n",
    "    logger.warning(\"The following columns in NUMERIC_FEATURES are absent from the df: {}\".format(\n",
    "        [col for col in NUMERIC_FEATURES if col not in df.columns],\n",
    "    ))\n",
    "if len([col for col in BOOLEAN_FEATURES if col not in df.columns]) > 0:\n",
    "    logger.warning(\"The following columns in BOOLEAN_FEATURES are absent from the df: {}\".format(\n",
    "        [col for col in BOOLEAN_FEATURES if col not in df.columns],\n",
    "    ))\n",
    "if len([col for col in CATEGORICAL_FEATURES if col not in df.columns]) > 0:\n",
    "    logger.warning(\"The following columns in CATEGORICAL_FEATURES are absent from the df: {}\".format(\n",
    "        [col for col in CATEGORICAL_FEATURES if col not in df.columns],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(\"object\").columns.tolist()\n",
    "num_cols = [\n",
    "    c for c in df.select_dtypes(\"number\").columns\n",
    "    if c not in {TARGET}\n",
    "]\n",
    "\n",
    "df_model = df[df[TARGET].notna()].copy()\n",
    "X = df_model[BOOLEAN_FEATURES + CATEGORICAL_FEATURES + NUMERIC_FEATURES]\n",
    "y = df_model[TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45705d71",
   "metadata": {},
   "source": [
    "# splitting training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d60c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create composite strata\n",
    "strata = (\n",
    "    df_model[\"seat_level_cong_recategorized\"].astype(str)\n",
    "    + \"__\"\n",
    "    + df_model[\"received_in_senate_political_era\"].astype(str)\n",
    ")\n",
    "\n",
    "# Optionally collapse very-rare strata to 'Other' (prevents ValueError)\n",
    "min_count = 3          # tweak as needed\n",
    "rare_mask = strata.map(strata.value_counts()) < min_count\n",
    "strata = strata.where(~rare_mask, other=\"Other\")\n",
    "\n",
    "# Trainâ€“test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=strata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm normally accustomed to naming python constants with all-caps, but my code assist tools are insisting on making these lowercase, so we'll alias them just to appease it\n",
    "numerical_features = NUMERIC_FEATURES\n",
    "categorical_features = CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d6351",
   "metadata": {},
   "source": [
    "# Model Selection, Training, and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f304ff9",
   "metadata": {},
   "source": [
    "##  Preprocessing pipeline setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d6e32",
   "metadata": {},
   "source": [
    "Given how much data cleaning is(n't) happening so far, it's entirely possible the test data will contain something the training pipeline has never seen in the training data.\n",
    "\n",
    "An example of an error message it threw when this occurred was `ValueError: Found unknown categories ['Syria', 'Russia', 'Trinidad and Tobago', 'Panama'] in column 4 during transform`, presumably because all of those were locations nominees have been appointed to work at or work with.\n",
    "\n",
    "The simplest workaround is to ignore them until/unless enough feature-engineering can be done to re-bin them (e.g. by continent, or job type, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0aecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(\n",
    "            drop='first', \n",
    "            sparse_output=False,\n",
    "            handle_unknown='ignore'  # ðŸ”§ workaround for error described in a markdown cell above\n",
    "        ), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16561816",
   "metadata": {},
   "source": [
    "## Untuned / naive pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6589d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(\n",
    "        tree_method=\"hist\",      # âœ… Current way\n",
    "        device=\"cuda\",           # âœ… Current way (replaces gpu_id, predictor, etc.)\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227ec69",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ff3cf",
   "metadata": {},
   "source": [
    "### Choice of metric: MAE\n",
    "\n",
    "Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) are both sensitive to outliers, and MSE also doesn't use the same units as our target variable, making it less intuitive.\n",
    "\n",
    "After all our data cleaning, we have few-enough rows of data, with outliers occurring often-enough, that I'm selecting Mean Absolute Error (MAE) as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-18 15:27:02.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.modeling.predict\u001b[0m:\u001b[36mtrain_and_evaluate_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mTraining XGBoost Baseline on 1424 samples, 43 features\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training XGBoost Baseline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-18 15:27:06.340\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.modeling.predict\u001b[0m:\u001b[36mtrain_and_evaluate_model\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mTraining completed in 3.45 seconds\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions:   0%|          | 0/2 [00:00<?, ?it/s]/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/core.py:729: UserWarning: [15:27:06] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "Making predictions:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  7.74it/s]/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [4, 9, 11, 12, 17, 19] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "Making predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate baseline model\n",
    "baseline_results = train_and_evaluate_model(\n",
    "    model=baseline_pipeline,  # âœ… Use the actual pipeline variable name\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    model_name=\"XGBoost Baseline\",\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3190f",
   "metadata": {},
   "source": [
    "## Saving with evaluation metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f7bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š XGBOOST BASELINE EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ PERFORMANCE METRICS:\n",
      "  â€¢ Training MAE: 10.46 days\n",
      "  â€¢ Test MAE:     47.05 days\n",
      "  â€¢ Training RÂ²:  0.9693\n",
      "  â€¢ Test RÂ²:      0.3599\n",
      "  â€¢ Training time: 3.45 seconds\n",
      "\n",
      "===== Mean Absolute Error (MAE): 47.05 =====\n",
      "ðŸ“Š GOOD: The model's predictions are typically within 60 days of the actual confirmation time.\n",
      "ðŸ” TAKEAWAY: The model provides valuable insights but has moderate error margins.\n",
      "\n",
      "===== RÂ² Score: 0.3599 =====\n",
      "ðŸ“Š FAIR: The model explains between 30-50% of the variance in confirmation times.\n",
      "ðŸ” TAKEAWAY: The model identifies some patterns but misses many important factors.\n",
      "\n",
      "===== Interpretation in Context =====\n",
      "â€¢ The average nomination takes 126 days to confirm\n",
      "â€¢ With a standard deviation of 85 days\n",
      "â€¢ Our model's error (MAE) is 47 days, which is 55% of the standard deviation\n",
      "â€¢ This means our model outperforms a baseline model that always predicts the average\n",
      "\n",
      "===== Recommended Next Steps =====\n",
      "1. Tune hyperparameters to optimize model performance\n",
      "2. Explore feature importance to understand key drivers\n",
      "3. Consider ensemble methods to improve predictions\n",
      "\n",
      "ðŸ”§ MODEL COMPLEXITY:\n",
      "Model complexity:\n",
      "  â€¢ Trees      : 500\n",
      "  â€¢ Leaf nodes : 0\n",
      "  â€¢ â€˜Parametersâ€™ (leaf weights) â‰ˆ 0.000000 B\n",
      "\u001b[32m2025-07-18 15:27:06.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.modeling.train\u001b[0m:\u001b[36msave_model_with_metadata\u001b[0m:\u001b[36m249\u001b[0m - \u001b[1mModel saved successfully to /home/wsl2ubuntuuser/nomination_predictor/models/xgboost_baseline_2025-07-18_152706.pkl\u001b[0m\n",
      "\u001b[32m2025-07-18 15:27:06.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.modeling.train\u001b[0m:\u001b[36msave_model_with_metadata\u001b[0m:\u001b[36m250\u001b[0m - \u001b[1mModel metadata: xgboost_baseline with 43 features\u001b[0m\n",
      "\n",
      "ðŸ’¾ Model saved to: /home/wsl2ubuntuuser/nomination_predictor/models/xgboost_baseline_2025-07-18_152706.pkl\n"
     ]
    }
   ],
   "source": [
    "baseline_final = evaluate_and_report_model(\n",
    "    results=baseline_results,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    model_name=\"XGBoost Baseline\",\n",
    "    save_model=True,\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 500,\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"random_state\": 42,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c68ac4",
   "metadata": {},
   "source": [
    "# Model tuning via randomized hyper-parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e21ac",
   "metadata": {},
   "source": [
    "## Define hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nomination_predictor.modeling.train import (PARAMETER_REFINEMENT_CONFIG,\n",
    "                                                 create_refined_param_grid,\n",
    "                                                 create_refined_range)\n",
    "\n",
    "# Choose your initial parameter distribution based on time budget\n",
    "# Option 1: Quick search (similar to your original - 11-12 minutes)\n",
    "param_distributions_quick = {\n",
    "    'model__n_estimators': [100, 500, 1000, 2000, 3000],\n",
    "    'model__max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'model__reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "    'model__reg_lambda': [0, 0.1, 0.5, 1.0],\n",
    "    'model__min_child_weight': [1, 3, 5, 7],\n",
    "    'model__gamma': [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Option 2: Moderate search (1-2 hours)\n",
    "param_distributions_moderate = {\n",
    "    'model__n_estimators': [100, 500, 1000, 2000, 3000, 5000],\n",
    "    'model__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "    'model__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'model__reg_alpha': [0, 0.1, 0.5, 1.0, 2.0],\n",
    "    'model__reg_lambda': [0, 0.1, 0.5, 1.0, 2.0],\n",
    "    'model__min_child_weight': [1, 3, 5, 7, 10],\n",
    "    'model__gamma': [0, 0.1, 0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Option 3: Comprehensive search (5-6 hours) - CLEANED UP VERSION\n",
    "param_distributions_comprehensive = {\n",
    "    'model__n_estimators': [100, 300, 500, 1000, 1500, 2000, 3000, 5000, 7000],\n",
    "    'model__max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 12],\n",
    "    'model__learning_rate': [0.005, 0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "    'model__subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'model__reg_alpha': [0, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'model__reg_lambda': [0, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    'model__min_child_weight': [1, 3, 5, 7, 10, 15],\n",
    "    'model__gamma': [0, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0],\n",
    "    # Note: Removed max_delta_step and scale_pos_weight as they're not typically used in regression\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3b54b",
   "metadata": {},
   "source": [
    "## Configurability based on available time budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ CHOOSE YOUR TIME BUDGET - uncomment one of these:\n",
    "# chosen_params = param_distributions_quick      # ~15 minutes\n",
    "chosen_params = param_distributions_moderate   # ~1-2 hours  \n",
    "# chosen_params = param_distributions_comprehensive  # ~5-6 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Selected parameter space with 9 parameters\n",
      "  model__n_estimators: 6 values\n",
      "  model__max_depth: 8 values\n",
      "  model__learning_rate: 6 values\n",
      "  model__subsample: 5 values\n",
      "  model__colsample_bytree: 5 values\n",
      "  model__reg_alpha: 5 values\n",
      "  model__reg_lambda: 5 values\n",
      "  model__min_child_weight: 5 values\n",
      "  model__gamma: 5 values\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ“Š Selected parameter space with {len(chosen_params)} parameters\")\n",
    "for param, values in chosen_params.items():\n",
    "    print(f\"  {param}: {len(values)} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439246d",
   "metadata": {},
   "source": [
    "## Tuned pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(\n",
    "        tree_method=\"hist\",        # GPU acceleration\n",
    "        device=\"cuda\",             # GPU acceleration  \n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        # No early stopping - let RandomizedSearchCV handle optimization; early stopping option may as well be mutually exclusive with multi-stage search\n",
    "        eval_metric='mae'          # Still monitor MAE for logging\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd0581",
   "metadata": {},
   "source": [
    "## Multi-stage randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting GPU-accelerated hyperparameter search...\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Starting GPU-accelerated hyperparameter search...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753453e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” STAGE 1: Coarse hyperparameter search...\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ STAGE 1: Coarse Search\n",
    "print(\"\\nðŸ” STAGE 1: Coarse hyperparameter search...\")\n",
    "coarse_search = RandomizedSearchCV(\n",
    "    tuned_pipeline,\n",
    "    param_distributions=chosen_params,\n",
    "    n_iter=60,  # Adjust based on your time budget\n",
    "    cv=3,       # Faster CV for initial search\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=1,   # Important for GPU\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6187e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸  Starting coarse search with 60 iterations...\n",
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=3, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.9; total time= -36.5s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=3, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=3, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=6, model__min_child_weight=10, model__n_estimators=1000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=6, model__min_child_weight=10, model__n_estimators=1000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=6, model__min_child_weight=10, model__n_estimators=1000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=2.0, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=2.0, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=2.0, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=7, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.7; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=7, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=7, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.05, model__max_depth=9, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.05, model__max_depth=9, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.05, model__max_depth=9, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.2, model__learning_rate=0.1, model__max_depth=9, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.2, model__learning_rate=0.1, model__max_depth=9, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.2, model__learning_rate=0.1, model__max_depth=9, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.1, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=3, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=1.0, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=3, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=1.0, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=3, model__min_child_weight=7, model__n_estimators=500, model__reg_alpha=1.0, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.1, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.1, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.1, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.15, model__max_depth=6, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.15, model__max_depth=6, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.15, model__max_depth=6, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=6, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.15, model__max_depth=6, model__min_child_weight=5, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.15, model__max_depth=6, model__min_child_weight=5, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.15, model__max_depth=6, model__min_child_weight=5, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.01, model__max_depth=3, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.01, model__max_depth=3, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.01, model__max_depth=3, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=0, model__subsample=0.8; total time=  36.7s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=7, model__min_child_weight=3, model__n_estimators=1000, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=7, model__min_child_weight=3, model__n_estimators=1000, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=7, model__min_child_weight=3, model__n_estimators=1000, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.05, model__max_depth=7, model__min_child_weight=7, model__n_estimators=100, model__reg_alpha=2.0, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.05, model__max_depth=7, model__min_child_weight=7, model__n_estimators=100, model__reg_alpha=2.0, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.05, model__max_depth=7, model__min_child_weight=7, model__n_estimators=100, model__reg_alpha=2.0, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=5, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=5, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=5, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=1, model__n_estimators=500, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=1, model__n_estimators=500, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=1, model__n_estimators=500, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.15, model__max_depth=8, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.15, model__max_depth=8, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.15, model__max_depth=8, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=3, model__min_child_weight=3, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=1.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=3, model__min_child_weight=3, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=1.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=3, model__min_child_weight=3, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=1.0, model__subsample=0.8; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=3, model__min_child_weight=1, model__n_estimators=100, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=3, model__min_child_weight=1, model__n_estimators=100, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=3, model__min_child_weight=1, model__n_estimators=100, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.05, model__max_depth=4, model__min_child_weight=5, model__n_estimators=2000, model__reg_alpha=1.0, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.05, model__max_depth=4, model__min_child_weight=5, model__n_estimators=2000, model__reg_alpha=1.0, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.05, model__max_depth=4, model__min_child_weight=5, model__n_estimators=2000, model__reg_alpha=1.0, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.1, model__max_depth=9, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.1, model__max_depth=9, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.1, model__max_depth=9, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0.1, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.3, model__max_depth=4, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.3, model__max_depth=4, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.3, model__max_depth=4, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=7, model__min_child_weight=3, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=7, model__min_child_weight=3, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=7, model__min_child_weight=3, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=9, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=9, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=9, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=6, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=6, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=6, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=10, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=10, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.3, model__learning_rate=0.1, model__max_depth=10, model__min_child_weight=5, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.2, model__learning_rate=0.3, model__max_depth=7, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.2, model__learning_rate=0.3, model__max_depth=7, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.2, model__learning_rate=0.3, model__max_depth=7, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=0.7; total time=  36.7s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=0.7; total time= -36.5s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=4, model__min_child_weight=7, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=4, model__min_child_weight=7, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.1, model__max_depth=4, model__min_child_weight=7, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=8, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=8, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=8, model__min_child_weight=5, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=10, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=10, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.3s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=10, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=0.1, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=2.0, model__reg_lambda=0.1, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.01, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.01, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.3, model__learning_rate=0.01, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.1, model__max_depth=5, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=2.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.1, model__max_depth=5, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=2.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0, model__learning_rate=0.1, model__max_depth=5, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0, model__reg_lambda=2.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.01, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.01, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.01, model__max_depth=9, model__min_child_weight=10, model__n_estimators=500, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0, model__learning_rate=0.05, model__max_depth=7, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0.1, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0, model__learning_rate=0.05, model__max_depth=7, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0.1, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0, model__learning_rate=0.05, model__max_depth=7, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0.1, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=1, model__n_estimators=500, model__reg_alpha=1.0, model__reg_lambda=2.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=1, model__n_estimators=500, model__reg_alpha=1.0, model__reg_lambda=2.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.3, model__learning_rate=0.3, model__max_depth=5, model__min_child_weight=1, model__n_estimators=500, model__reg_alpha=1.0, model__reg_lambda=2.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.1, model__learning_rate=0.2, model__max_depth=4, model__min_child_weight=1, model__n_estimators=100, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.1, model__learning_rate=0.2, model__max_depth=4, model__min_child_weight=1, model__n_estimators=100, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.1, model__learning_rate=0.2, model__max_depth=4, model__min_child_weight=1, model__n_estimators=100, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.2, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.2, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.2, model__learning_rate=0.15, model__max_depth=4, model__min_child_weight=7, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=0.9; total time=  36.7s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=10, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=2.0, model__subsample=0.6; total time= -36.5s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=10, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=2.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=10, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=2.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=10, model__n_estimators=1000, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=10, model__n_estimators=1000, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=10, model__n_estimators=1000, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.1, model__max_depth=5, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.1, model__max_depth=5, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0, model__learning_rate=0.1, model__max_depth=5, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=0, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=1, model__n_estimators=1000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=1, model__n_estimators=1000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.3, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=1, model__n_estimators=1000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=4, model__min_child_weight=10, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=4, model__min_child_weight=10, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.05, model__max_depth=4, model__min_child_weight=10, model__n_estimators=5000, model__reg_alpha=2.0, model__reg_lambda=0.5, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=3, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=3, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=3, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=0.1, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=7, model__n_estimators=5000, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=7, model__n_estimators=5000, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=9, model__min_child_weight=7, model__n_estimators=5000, model__reg_alpha=0.1, model__reg_lambda=0.5, model__subsample=0.9; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=7, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=7, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0.1, model__learning_rate=0.2, model__max_depth=5, model__min_child_weight=7, model__n_estimators=2000, model__reg_alpha=0.5, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.1, model__learning_rate=0.15, model__max_depth=3, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=1.0, model__reg_lambda=0.5, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.1, model__learning_rate=0.15, model__max_depth=3, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=1.0, model__reg_lambda=0.5, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.1, model__learning_rate=0.15, model__max_depth=3, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=1.0, model__reg_lambda=0.5, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=6, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=1.0, model__reg_lambda=0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=6, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=1.0, model__reg_lambda=0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.9, model__gamma=0.5, model__learning_rate=0.3, model__max_depth=6, model__min_child_weight=10, model__n_estimators=2000, model__reg_alpha=1.0, model__reg_lambda=0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.1, model__learning_rate=0.15, model__max_depth=9, model__min_child_weight=10, model__n_estimators=100, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.2s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.1, model__learning_rate=0.15, model__max_depth=9, model__min_child_weight=10, model__n_estimators=100, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.1, model__learning_rate=0.15, model__max_depth=9, model__min_child_weight=10, model__n_estimators=100, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=8, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=8, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.2, model__learning_rate=0.2, model__max_depth=8, model__min_child_weight=1, model__n_estimators=5000, model__reg_alpha=0.5, model__reg_lambda=0.5, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.1, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.1, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.6, model__gamma=0.1, model__learning_rate=0.01, model__max_depth=7, model__min_child_weight=5, model__n_estimators=1000, model__reg_alpha=0, model__reg_lambda=1.0, model__subsample=1.0; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=7, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=7, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.8, model__gamma=0.5, model__learning_rate=0.2, model__max_depth=7, model__min_child_weight=10, model__n_estimators=3000, model__reg_alpha=0, model__reg_lambda=0.1, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.01, model__max_depth=6, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.01, model__max_depth=6, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.7; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0.2, model__learning_rate=0.01, model__max_depth=6, model__min_child_weight=1, model__n_estimators=2000, model__reg_alpha=2.0, model__reg_lambda=1.0, model__subsample=0.7; total time=  36.7s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.3, model__max_depth=8, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.5, model__reg_lambda=1.0, model__subsample=0.6; total time= -36.5s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.3, model__max_depth=8, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.5, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=1.0, model__gamma=0, model__learning_rate=0.3, model__max_depth=8, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.5, model__reg_lambda=1.0, model__subsample=0.6; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n",
      "[CV] END model__colsample_bytree=0.7, model__gamma=0, model__learning_rate=0.2, model__max_depth=10, model__min_child_weight=1, model__n_estimators=3000, model__reg_alpha=0.1, model__reg_lambda=2.0, model__subsample=0.8; total time=   0.1s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 180 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n180 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/base.py\", line 1363, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/pipeline.py\", line 661, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1247, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/callback.py\", line 267, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/callback.py\", line 267, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/callback.py\", line 463, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâ±ï¸  Starting coarse search with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoarse_search.n_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcoarse_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Stage 1 complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“ˆ Best coarse score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m-coarse_search.best_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MAE\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1028\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m   1026\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 180 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n180 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/base.py\", line 1363, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/sklearn/pipeline.py\", line 661, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/sklearn.py\", line 1247, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/core.py\", line 729, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/training.py\", line 184, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/callback.py\", line 267, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/callback.py\", line 267, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/wsl2ubuntuuser/.virtualenvs/nomination_predictor/lib/python3.12/site-packages/xgboost/callback.py\", line 463, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n"
     ]
    }
   ],
   "source": [
    "print(f\"â±ï¸  Starting coarse search with {coarse_search.n_iter} iterations...\")\n",
    "coarse_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"âœ… Stage 1 complete!\")\n",
    "print(f\"ðŸ“ˆ Best coarse score: {-coarse_search.best_score_:.2f} MAE\")\n",
    "print(f\"ðŸŽ¯ Best parameters from coarse search:\")\n",
    "for param, value in coarse_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_search.fit(X_train, y_train)\n",
    "print(f\"âœ… Stage 1 complete! Best score: {-coarse_search.best_score_:.2f} MAE\")\n",
    "print(f\"ðŸ“Š Best parameters from coarse search:\")\n",
    "for param, value in coarse_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ STAGE 2: Refined Search using helper functions\n",
    "print(\"\\nðŸ”§ STAGE 2: Fine-tuned search around best parameters...\")\n",
    "\n",
    "# Use your helper function to create refined parameter grid\n",
    "refined_params = create_refined_param_grid(\n",
    "    best_params=coarse_search.best_params_,\n",
    "    num_points=5  # Number of values to try around each best parameter\n",
    ")\n",
    "\n",
    "print(f\"ðŸ” Refined parameter space:\")\n",
    "for param, values in refined_params.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Calculate smart n_iter for refined search\n",
    "total_combinations = len(list(itertools.product(*refined_params.values())))\n",
    "n_iter_fine = min(100, total_combinations)  # Don't exceed total combinations\n",
    "\n",
    "print(f\"ðŸ“Š Fine search: {n_iter_fine} iterations out of {total_combinations} possible combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a497bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Fine search\n",
    "fine_search = RandomizedSearchCV(\n",
    "    tuned_pipeline,\n",
    "    param_distributions=refined_params,\n",
    "    n_iter=n_iter_fine,\n",
    "    cv=5,       # More thorough CV for final search\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=1,\n",
    "    random_state=43,  # Different seed for diversity\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7559e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"â±ï¸  Starting fine search...\")\n",
    "fine_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"âœ… Stage 2 complete!\")\n",
    "print(f\"ðŸ“ˆ Best fine score: {-fine_search.best_score_:.2f} MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Compare results\n",
    "coarse_score = -coarse_search.best_score_\n",
    "fine_score = -fine_search.best_score_\n",
    "improvement = ((coarse_score - fine_score) / coarse_score) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nðŸŽ¯ MULTI-STAGE SEARCH RESULTS:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"  Coarse search best MAE: {coarse_score:.2f} days\")\n",
    "print(f\"  Fine search best MAE:   {fine_score:.2f} days\")\n",
    "if improvement > 0:\n",
    "    print(f\"  ðŸŽ‰ Improvement: {improvement:.1f}% ({coarse_score:.2f} â†’ {fine_score:.2f})\")\n",
    "else:\n",
    "    print(f\"  ðŸ“Š Change: {improvement:.1f}% (fine search: {fine_score:.2f})\")\n",
    "\n",
    "print(f\"\\nðŸ† Final best parameters:\")\n",
    "for param, value in fine_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af087f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model from fine search for final evaluation\n",
    "best_model_final = fine_search.best_estimator_\n",
    "best_params_final = fine_search.best_params_\n",
    "\n",
    "print(f\"\\nâœ… Multi-stage hyperparameter optimization complete!\")\n",
    "print(f\"ðŸŽ¯ Ready for final model evaluation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f70c6d7",
   "metadata": {},
   "source": [
    "## Evaluating best model found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_results = train_and_evaluate_model(\n",
    "    model=best_model_final,  # Use the best model from fine search\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    model_name=\"XGBoost Multi-Stage Tuned\",\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57682a53",
   "metadata": {},
   "source": [
    "## Saving and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and evaluate\n",
    "tuned_final = evaluate_and_report_model(\n",
    "    results=tuned_results,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    model_name=\"XGBoost Multi-Stage Tuned\",\n",
    "    save_model=True,\n",
    "    hyperparameters=best_params_final\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70096f6a",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d869b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models using our comparison function\n",
    "compare_models(\n",
    "    results_list=[baseline_results, tuned_results],\n",
    "    model_names=[\"XGBoost Baseline\", \"XGBoost Multi-Stage Tuned\"]\n",
    ")\n",
    "\n",
    "# Display model paths for reference\n",
    "print(f\"\\nðŸ“ SAVED MODELS:\")\n",
    "print(f\"  â€¢ Baseline: {baseline_final['model_path']}\")\n",
    "print(f\"  â€¢ Tuned:    {tuned_final['model_path']}\")\n",
    "\n",
    "# Show interpretation categories\n",
    "print(f\"\\nðŸ·ï¸ MODEL QUALITY ASSESSMENT:\")\n",
    "print(f\"  â€¢ Baseline: {baseline_final['interpretation']['overall_quality']}\")\n",
    "print(f\"  â€¢ Tuned:    {tuned_final['interpretation']['overall_quality']}\")\n",
    "\n",
    "# Feature importance analysis (if desired)\n",
    "best_model = tuned_final['model']\n",
    "if hasattr(best_model, 'feature_importances_') or hasattr(best_model[-1], 'feature_importances_'):\n",
    "    print(f\"\\nðŸ” TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = best_model[:-1].get_feature_names_out()\n",
    "    \n",
    "    # Get feature importances\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "    else:\n",
    "        importances = best_model[-1].feature_importances_\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Display top 10\n",
    "    for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows()):\n",
    "        print(f\"  {i+1:2d}. {row['feature']:<30} {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5324e6b",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d085e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Actual vs Predicted (Baseline)\n",
    "axes[0, 0].scatter(y_test, baseline_results['predictions']['y_test_pred'], alpha=0.6)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Days')\n",
    "axes[0, 0].set_ylabel('Predicted Days')\n",
    "axes[0, 0].set_title(f'Baseline Model\\nMAE: {baseline_results[\"metrics\"][\"test_mae\"]:.2f} days')\n",
    "\n",
    "# 2. Actual vs Predicted (Tuned)\n",
    "axes[0, 1].scatter(y_test, tuned_results['predictions']['y_test_pred'], alpha=0.6)\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Days')\n",
    "axes[0, 1].set_ylabel('Predicted Days')\n",
    "axes[0, 1].set_title(f'Tuned Model\\nMAE: {tuned_results[\"metrics\"][\"test_mae\"]:.2f} days')\n",
    "\n",
    "# 3. Residuals (Baseline)\n",
    "baseline_residuals = y_test - baseline_results['predictions']['y_test_pred']\n",
    "axes[1, 0].scatter(baseline_results['predictions']['y_test_pred'], baseline_residuals, alpha=0.6)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Predicted Days')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Baseline Residuals')\n",
    "\n",
    "# 4. Residuals (Tuned)\n",
    "tuned_residuals = y_test - tuned_results['predictions']['y_test_pred']\n",
    "axes[1, 1].scatter(tuned_results['predictions']['y_test_pred'], tuned_residuals, alpha=0.6)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Predicted Days')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Tuned Residuals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomination_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
