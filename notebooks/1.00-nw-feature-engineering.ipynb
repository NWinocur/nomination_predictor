{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd428ca2",
   "metadata": {},
   "source": [
    "# Notebook 1 – Data Cleaning, Feature Engineering, & Entity Resolution\n",
    "**Project:** Judicial Vacancy → Nomination/Confirmation Pipeline\n",
    "\n",
    "*Initial draft generated via ChatGPT model o3 on 2025-07-12T02:40:38.399372Z*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# Add the project root to the path so we can import our modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logger.remove()  # Remove default handler\n",
    "logger.add(sys.stderr, format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{function}</cyan> - <level>{message}</level>\", level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b06c6",
   "metadata": {},
   "source": [
    "## Load dataframes from Raw data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cff186",
   "metadata": {},
   "source": [
    "Start with loading simpler, non-JSON-containing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d74987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomination_predictor.config import INTERIM_DATA_DIR, RAW_DATA_DIR\n",
    "\n",
    "# load FJC dataframes (and derived seat timeline)\n",
    "fjc_judges = pd.read_csv(RAW_DATA_DIR / \"judges.csv\")\n",
    "fjc_federal_judicial_service = pd.read_csv(RAW_DATA_DIR / \"federal_judicial_service.csv\")\n",
    "fjc_demographics = pd.read_csv(RAW_DATA_DIR / \"demographics.csv\")\n",
    "fjc_education = pd.read_csv(RAW_DATA_DIR / \"education.csv\")\n",
    "fjc_other_federal_judicial_service = pd.read_csv(\n",
    "    RAW_DATA_DIR / \"other_federal_judicial_service.csv\"\n",
    ")\n",
    "fjc_other_nominations_recess = pd.read_csv(RAW_DATA_DIR / \"other_nominations_recess.csv\")\n",
    "seat_timeline = pd.read_csv(RAW_DATA_DIR / \"seat_timeline.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8b36b",
   "metadata": {},
   "source": [
    "Flatten JSON-containing congress DataFrames into separate DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomination_predictor.features import flatten_json_dataframe\n",
    "\n",
    "# Load Congress API dataframes\n",
    "cong_nominations_raw = pd.read_csv(RAW_DATA_DIR / \"nominations.csv\")\n",
    "cong_nominees_raw = pd.read_csv(RAW_DATA_DIR / \"nominees.csv\")\n",
    "\n",
    "cong_nominations = flatten_json_dataframe(\n",
    "    df=cong_nominations_raw,\n",
    "    json_col=\"nomination\",  # column containing the JSON data\n",
    "    max_list_index=10,      # maximum number of list items to extract\n",
    "    separator=\"_\"           # separator for nested keys\n",
    ")\n",
    "\n",
    "cong_nominees= flatten_json_dataframe(\n",
    "    df=cong_nominees_raw,\n",
    "    json_col=\"nominee\",\n",
    "    max_list_index=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9765e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes into a single dictionary for bulk operations\n",
    "# Start with FJC dataframes\n",
    "dfs = {\n",
    "    # FJC dataframes\n",
    "    \"fjc_judges\": fjc_judges,\n",
    "    \"fjc_federal_judicial_service\": fjc_federal_judicial_service,\n",
    "    \"fjc_demographics\": fjc_demographics,\n",
    "    \"fjc_education\": fjc_education,\n",
    "    \"fjc_other_federal_judicial_service\": fjc_other_federal_judicial_service,\n",
    "    \"fjc_other_nominations_recess\": fjc_other_nominations_recess,\n",
    "    \"seat_timeline\": seat_timeline,\n",
    "    \n",
    "    # Congress dataframes\n",
    "    \"cong_nominations\": cong_nominations,\n",
    "    \"cong_nominees\": cong_nominees,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of available dataframes\n",
    "print(\"Available dataframes:\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"- {name}: {len(df)} rows × {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18237c18",
   "metadata": {},
   "source": [
    "Cong_nominee_orgs and cong_nominee_edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4086e",
   "metadata": {},
   "source": [
    "JSON-containing files we can explode and/or flatten several different ways.  Whichever one is best depends on the use case.  Below is the method I settled on so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947abcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented out because function this calls would throw warnings for what is by now known and tolerated table conditions\n",
    "\n",
    "#from nomination_predictor.dataset import check_id_uniqueness\n",
    "## Check each DataFrame for uniqueness of citation field\n",
    "#print(\"Checking uniqueness of nomination/nominee identifiers...\")\n",
    "#for name, df in dfs.items():\n",
    "#    if name.startswith(\"fjc_\"):\n",
    "#        logger.info(f\"\\n- Checking {name}...\")\n",
    "#        col=\"nid\"\n",
    "#        if col in df.columns:\n",
    "#            check_id_uniqueness(df, id_field=col)\n",
    "#        else:\n",
    "#            logger.info(f\"  Skipped: {col} column not found in {name}\")\n",
    "#    if name.startswith(\"cong_\"):\n",
    "#        logger.info(f\"\\n- Checking {name}...\")\n",
    "#        col=\"citation\"\n",
    "#        if col in df.columns:\n",
    "#            check_id_uniqueness(df, id_field=col)\n",
    "#        else:\n",
    "#            logger.info(f\"  Skipped: {col} column not found in {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87162502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented this cell out because IMO it's too early in this notebook to be worthwhile to save these as CSVs\n",
    "\n",
    "## Save extracted tables to interim directory\n",
    "#for name, df in dfs.items():\n",
    "#    if len(df) > 0:  # Only save non-empty DataFrames\n",
    "#        output_path = INTERIM_DATA_DIR / f\"{name}.csv\"\n",
    "#        df.to_csv(output_path, index=False)\n",
    "#        print(f\"Saved {len(df)} records to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682d66f",
   "metadata": {},
   "source": [
    "#### Quick peek at all loaded dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Checking for general shape and first handfuls of rows\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name:<35} → {df.shape}\")\n",
    "    print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Checking for null values\")\n",
    "    \n",
    "for name, df in dfs.items():\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f541a22",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a3d80",
   "metadata": {},
   "source": [
    "## Normalize column names for DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0828397",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Column Names Before ===\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name:<35} → {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01227ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call features.py's normalize_columns function on all DataFrames in dfs, and strip leading and trailing whitespace in all strings\n",
    "from nomination_predictor.features import normalize_dataframe_columns\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df = normalize_dataframe_columns(df)\n",
    "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b405378",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Column Names After ===\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name:<35} → {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34e1e1",
   "metadata": {},
   "source": [
    "Left-merge nominees table onto nominations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomination_predictor.features import merge_nominees_onto_nominations\n",
    "\n",
    "try:\n",
    "    # Assuming cong_nominations and cong_nominees dataframes are already loaded\n",
    "    cong_noms = merge_nominees_onto_nominations(dfs[\"cong_nominations\"], dfs[\"cong_nominees\"])\n",
    "    \n",
    "    # Show sample of the merged dataframe\n",
    "    display(cong_noms.head())\n",
    "    \n",
    "    # Report on the merge results\n",
    "    logger.info(f\"Original nominations shape: {cong_nominations.shape}\")\n",
    "    logger.info(f\"Original nominees shape: {cong_nominees.shape}\")\n",
    "    logger.info(f\"Merged dataframe shape: {cong_noms.shape}\")\n",
    "    \n",
    "    dfs[\"cong_noms\"] = cong_noms\n",
    "    \n",
    "except NameError:\n",
    "    logger.error(\"Required dataframes (cong_nominations, cong_nominees) are not defined\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in merge process: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24052def",
   "metadata": {},
   "source": [
    "### Drop non-judge nominations based on position title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-judicial nominations using the function from features.py\n",
    "from nomination_predictor.features import filter_non_judicial_nominations\n",
    "\n",
    "# Define non-judicial titles to filter out\n",
    "non_judicial_titles = [\n",
    "    \"Attorney\", \"Board\", \"Commission\", \"Director\", \"Marshal\",\n",
    "    \"Assistant\", \"Representative\", \"Secretary of\", \"Member of\"\n",
    "]\n",
    "\n",
    "dfs[\"cong_noms\"] = filter_non_judicial_nominations(dfs[\"cong_noms\"], non_judicial_titles=non_judicial_titles)\n",
    "cong_noms = dfs[\"cong_noms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b269eea",
   "metadata": {},
   "source": [
    "### Convert date strings to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01904b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for any columns which contain certain keywords in their column name and contain string values, convert from string to datetime\n",
    "datetime_related_keywords = (\"date\", \"year\", \"month\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col for keyword in datetime_related_keywords) and df[col].dtype == \"object\":\n",
    "            logger.info(f\"Converting {col} to datetime for {name}\")\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb8848",
   "metadata": {},
   "source": [
    "### Normalize all string values we'll later have to fuzzy-match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_which_denote_string_columns_to_normalize = (\"court\", \"circuit\", \"district\", \"description\", \"name\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.casefold() for keyword in keywords_which_denote_string_columns_to_normalize) and df[col].dtype == object:\n",
    "            logger.info(F\"Normalizing all values within column named {col} in {name}\")\n",
    "            df[col] = df[col].str.casefold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5071ca",
   "metadata": {},
   "source": [
    "### Count and display unique values under each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2df09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display counts of unique values in DataFrame columns:\n",
    "for name, df in dfs.items():\n",
    "    for col in sorted(df.columns):\n",
    "     print(f\"{name} - {col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba6742",
   "metadata": {},
   "source": [
    "### Set nid as index (for the couple of FJC dataframes designed to use 'nid' uniquely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the dataframes that have unique nid, set them as the index to optimize lookups/joins\n",
    "dfs[\"fjc_judges\"].set_index('nid', drop=False, inplace=True, verify_integrity=True)\n",
    "dfs[\"fjc_demographics\"].set_index('nid', drop=False, inplace=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558daca8",
   "metadata": {},
   "source": [
    "## Fuzzy-matching FJC judges to Congress.gov nominees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615091a8",
   "metadata": {},
   "source": [
    "### Preparing columns to aid matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a \"full_name_concatenated\" column to the fjc_federal_judicial_service dataframe which is composed by flipping its judge_name column values \n",
    "# from \"lastname, firstname middleNameOrMiddleInitial (, optional comma and suffix)\" to \"firstname lastname middle suffix\"\n",
    "from nomination_predictor.features import \\\n",
    "    convert_judge_name_format_from_last_comma_first_to_first_then_last\n",
    "\n",
    "try:\n",
    "    dfs[\"fjc_federal_judicial_service\"][\"full_name_concatenated\"] = dfs[\"fjc_federal_judicial_service\"][\"judge_name\"].apply(convert_judge_name_format_from_last_comma_first_to_first_then_last)\n",
    "    \n",
    "    # Show some examples to verify the conversion\n",
    "    sample = dfs[\"fjc_federal_judicial_service\"][['judge_name', 'full_name_concatenated']].head(10)\n",
    "    display(sample)\n",
    "    \n",
    "    # Count null values to check for any conversion failures\n",
    "    null_count = dfs[\"fjc_federal_judicial_service\"][\"full_name_concatenated\"].isna().sum()\n",
    "    empty_count = (dfs[\"fjc_federal_judicial_service\"][\"full_name_concatenated\"] == '').sum()\n",
    "    \n",
    "    if null_count > 0 or empty_count > 0:\n",
    "        print(f\"Warning: Found {null_count} null values and {empty_count} empty strings in the converted names.\")\n",
    "        \n",
    "    print(f\"Successfully added 'full_name_concatenated' column to fjc_federal_judicial_service dataframe with {len(dfs[\"fjc_federal_judicial_service\"])}) entries.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating full_name_concatenated column: {e}\")\n",
    "    # If there's an error, display the first few rows of fjc_federal_judicial_service to help diagnose\n",
    "    logger.info(\"\\nSample of fjc_federal_judicial_service dataframe:\")\n",
    "    display(dfs[\"fjc_federal_judicial_service\"].head(3))\n",
    "    logger.info(f\"Columns available: {dfs[\"fjc_federal_judicial_service\"].columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a \"full_name_from_description\" and a \"location_of_origin_from_description\" columns to the dfs[\"cong_noms\"] dataframe which regex-captures the first segments of the same dfs[\"cong_noms\"] dataframe row's \"description\" string, \n",
    "# i.e. captures name before the first appearances of the phrases \", of \" or \", of the \"\n",
    "# and captures location from the second segment of the same dfs[\"cong_noms\"] dataframe row's \"description\" string\n",
    "# i.e. captures between the above-seen phrase \", of \" or \", of the \" through to the phrase \", to be \"\n",
    "# examples: \n",
    "# melissa damian, of florida, to be ...  gets captured into those new columns as \"melissa damian\" and \"florida\"\n",
    "# nicole g. bernerr of maryland, to be united... gets captured into those new columns as \"nicole g. bernerr\" and \"maryland\"\n",
    "# kirk edward sherriff, of california, to be united... gets captured into those new columns as \"kirk edward sherriff\" and \"california\"\n",
    "# sherri malloy beatty-arthur, of the district of columbia, for... gets captured into those new columns as \"sherri malloy beatty-arthur\" and \"district of columbia\"\n",
    "\n",
    "# Extract full_name_from_description and location_of_origin_from_description from description field\n",
    "from nomination_predictor.features import extract_name_and_location_columns\n",
    "\n",
    "# Apply the extraction function to cong_noms dataframe\n",
    "if 'cong_noms' in dfs:\n",
    "    dfs['cong_noms'] = extract_name_and_location_columns(dfs['cong_noms'])\n",
    "    \n",
    "    # Display sample results to verify extraction\n",
    "    sample_cols = ['description', 'full_name_from_description', 'location_of_origin_from_description']\n",
    "    display(dfs['cong_noms'][sample_cols].head(10))\n",
    "    \n",
    "    # Report extraction statistics\n",
    "    total_rows = len(dfs['cong_noms'])\n",
    "    name_filled = dfs['cong_noms']['full_name_from_description'].notna().sum()\n",
    "    location_filled = dfs['cong_noms']['location_of_origin_from_description'].notna().sum()\n",
    "    \n",
    "    print(f\"Extracted names for {name_filled}/{total_rows} records ({name_filled/total_rows:.1%})\")\n",
    "    print(f\"Extracted locations for {location_filled}/{total_rows} records ({location_filled/total_rows:.1%})\")\n",
    "else:\n",
    "    print(\"Error: 'cong_noms' dataframe not found in dfs dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomination_predictor.name_matching import perform_exact_name_matching\n",
    "\n",
    "results_of_name_matching_to_bridge_nids_to_congress_dataframe_indices= perform_exact_name_matching(\n",
    "    congress_df=dfs[\"cong_noms\"],\n",
    "    fjc_df=dfs[\"fjc_federal_judicial_service\"],\n",
    "    congress_name_col=\"full_name_from_description\",\n",
    "    fjc_name_col=\"judge_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e68516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "results_of_name_matching_to_bridge_nids_to_congress_dataframe_indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db338ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only *unambiguous* pairs\n",
    "nid_map = (\n",
    "    results_of_name_matching_to_bridge_nids_to_congress_dataframe_indices[~results_of_name_matching_to_bridge_nids_to_congress_dataframe_indices[\"ambiguous\"]]        # drop rows still ambiguous\n",
    "      .set_index(\"congress_index\")[\"nid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at long last, we have a way to bridge the gap between the congress.gov data and the fjc data\n",
    "\n",
    "# we can now use the nid_map to add the nid column to the congress.gov data\n",
    "dfs[\"cong_noms\"][\"nid\"] = dfs[\"cong_noms\"].index.to_series().map(nid_map)\n",
    "cong_noms = dfs[\"cong_noms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: decide whether to save as separate vs. overwrite in interim folder\n",
    "## Save the cleaned interim datasets for downstream notebooks\n",
    "#cong_nominees.to_csv(INTERIM_DATA_DIR / \"congress_nominees_cleaned.csv\", index=False)\n",
    "#fjc_judges.to_csv(INTERIM_DATA_DIR / \"fjc_judges_cleaned.csv\", index=False)\n",
    "#fjc_service.to_csv(INTERIM_DATA_DIR / \"fjc_service_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcfcbf",
   "metadata": {},
   "source": [
    "## Combining FJC data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7daa8b",
   "metadata": {},
   "source": [
    "### Handling nominees' education and job history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74656c7d",
   "metadata": {},
   "source": [
    "Before we combine FJC data, we have to consider whether/how to handle judges' education, job history, age, ABA rating, etc., because the only other table in the FJC data which handles nid uniquely is \"demographics,\" which are unchanging.\n",
    "The simplest way to handle the non-unique-nid tables it would be to left-merge on \"nid\" and only take the most recently-dated row.  In most cases this would likely land on keeping the most prestigious degree or job.\n",
    "\n",
    "However, it is entirely likely a judge's education or job history has changed substantially since their first nomination, and affected their qualifications for each later nomination.\n",
    "\n",
    "All of these indicate to me that it's worth considering the judge's position, education, etc., not as of the most recent records available, but instead _as of when they were nominated._\n",
    "\n",
    "That means we can't do a simple left-join of all of our FJC data.  Instead, we have to -- using a combination of names, court locations, and vacancy dates -- fuzzy-match to find which \"nid\" corresponds to each \"citation\" in the Congress data, as our way of bridging between FJC judges and congress' nominee data. Then use the \"received date\" for that citation as a cutoff date for when we lookup education and job records by \"nid\" -- so we can avoid mistakenly linking to a citation any employemnt & job records dated after that cutoff date.\n",
    "\n",
    "Thankfully we do have the school, degree, and degree_year in the education record, for both their bachelors and their masters and their associate degree(s) and LLB and J.D. etc., so we can look that up.  The education dataframe even comes with a \"sequence\" number for each education record, which is an even easier-to-use indicator of chronological order than the degree_year for any given \"nid\" lookup for a judge.\n",
    "\n",
    "Job history is more challenging to deal with because literally every row entry in that dataframe lists it uniquely, but we do have the data available.  On early attempts, it may be simplest to ignore it; then feature-engineer basic booleans for whether they did/didn't have experience in common-phrase-identifiable positions such as \"Private practice\" or \"Attorney general\" or \"Navy\" or \"Army\" etc.; eventually a parser can look for the year spreads listed there as a rough indicator of amounts of experience gleaned from each professional role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd9fe3",
   "metadata": {},
   "source": [
    "### Build predecessor lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predecessor lookup table\n",
    "predecessor_lookup = get_predecessor_info(seat_timeline_df)\n",
    "print(f\"Created predecessor lookup: {len(predecessor_lookup)} records\")\n",
    "\n",
    "# Preview the predecessor lookup\n",
    "print(predecessor_lookup.head())\n",
    "all_dataframes['predecessor_lookup'] = predecessor_lookup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomination_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
