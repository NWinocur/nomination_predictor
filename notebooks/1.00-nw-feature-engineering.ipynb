{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd428ca2",
   "metadata": {},
   "source": [
    "# Notebook 1 – Data Cleaning, Feature Engineering, & Entity Resolution\n",
    "**Project:** Judicial Vacancy → Nomination/Confirmation Pipeline\n",
    "\n",
    "*Initial draft generated via ChatGPT model o3 on 2025-07-12T02:40:38.399372Z*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69ea97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# Add the project root to the path so we can import our modules\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logger.remove()  # Remove default handler\n",
    "logger.add(sys.stderr, format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{function}</cyan> - <level>{message}</level>\", level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b06c6",
   "metadata": {},
   "source": [
    "## Load dataframes from Raw data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cff186",
   "metadata": {},
   "source": [
    "Start with loading simpler, non-JSON-containing CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d74987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 12:02:27.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mProject root: /home/wsl2ubuntuuser/nomination_predictor\u001b[0m\n",
      "\u001b[32m2025-07-13 12:02:27.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mConfiguration loaded\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nomination_predictor.config import INTERIM_DATA_DIR, RAW_DATA_DIR\n",
    "\n",
    "# load FJC dataframes (and derived seat timeline)\n",
    "fjc_judges = pd.read_csv(RAW_DATA_DIR / \"judges.csv\")\n",
    "fjc_federal_judicial_service = pd.read_csv(RAW_DATA_DIR / \"federal_judicial_service.csv\")\n",
    "fjc_demographics = pd.read_csv(RAW_DATA_DIR / \"demographics.csv\")\n",
    "fjc_education = pd.read_csv(RAW_DATA_DIR / \"education.csv\")\n",
    "fjc_other_federal_judicial_service = pd.read_csv(\n",
    "    RAW_DATA_DIR / \"other_federal_judicial_service.csv\"\n",
    ")\n",
    "fjc_other_nominations_recess = pd.read_csv(RAW_DATA_DIR / \"other_nominations_recess.csv\")\n",
    "seat_timeline = pd.read_csv(RAW_DATA_DIR / \"seat_timeline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Congress API dataframes\n",
    "cong_nominations = pd.read_csv(RAW_DATA_DIR / \"nominations.csv\")\n",
    "cong_nominees = pd.read_csv(RAW_DATA_DIR / \"nominees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8b36b",
   "metadata": {},
   "source": [
    "Explode JSON-containing congress DataFrames into separate DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 12:02:28.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.features\u001b[0m:\u001b[36mexplode_nomination_json\u001b[0m:\u001b[36m665\u001b[0m - \u001b[1mProcessing 5746 nomination records\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting JSON data: 100%|██████████| 5746/5746 [00:13<00:00, 426.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 12:02:41.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.features\u001b[0m:\u001b[36mexplode_nomination_json\u001b[0m:\u001b[36m834\u001b[0m - \u001b[1mExtracted 5746 nomination records\u001b[0m\n",
      "\u001b[32m2025-07-13 12:02:41.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.features\u001b[0m:\u001b[36mexplode_nomination_json\u001b[0m:\u001b[36m835\u001b[0m - \u001b[1mExtracted 0 nominee records\u001b[0m\n",
      "\u001b[32m2025-07-13 12:02:41.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.features\u001b[0m:\u001b[36mexplode_nomination_json\u001b[0m:\u001b[36m836\u001b[0m - \u001b[1mExtracted 5513 action records\u001b[0m\n",
      "\u001b[32m2025-07-13 12:02:41.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.features\u001b[0m:\u001b[36mexplode_nomination_json\u001b[0m:\u001b[36m837\u001b[0m - \u001b[1mExtracted 5453 committee records\u001b[0m\n",
      "\u001b[32m2025-07-13 12:02:41.587\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomination_predictor.features\u001b[0m:\u001b[36mexplode_nomination_json\u001b[0m:\u001b[36m838\u001b[0m - \u001b[1mExtracted 864 hearing records\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exploded_nom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m exploded_nominations = explode_nomination_json(cong_nominations)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Extract each dataframe as a separate variable for IDE inspection\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m cong_nomination_core = \u001b[43mexploded_nom\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mnominations\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m cong_nomination_nominees = exploded_nom[\u001b[33m'\u001b[39m\u001b[33mnominees\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m cong_nomination_actions = exploded_nom[\u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'exploded_nom' is not defined"
     ]
    }
   ],
   "source": [
    "from nomination_predictor.features import (explode_nomination_json,\n",
    "                                           explode_nominee_json)\n",
    "\n",
    "# Explode nomination JSON data\n",
    "exploded_nominations = explode_nomination_json(cong_nominations)\n",
    "# Extract each dataframe as a separate variable for IDE inspection\n",
    "cong_nomination_core = exploded_nom['nominations']\n",
    "cong_nomination_nominees = exploded_nom['nominees']\n",
    "cong_nomination_actions = exploded_nom['actions']\n",
    "cong_nomination_committees = exploded_nom['committees']\n",
    "cong_nomination_hearings = exploded_nom['hearings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052140ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode nominee JSON data\n",
    "exploded_nominee = explode_nominee_json(cong_nominees)\n",
    "# Extract each dataframe as a separate variable for IDE inspection\n",
    "cong_nominee_core = exploded_nominee['nominees']\n",
    "cong_nominee_orgs = exploded_nominee['organizations']\n",
    "cong_nominee_edu = exploded_nominee['educational_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9765e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes into a single dictionary for bulk operations\n",
    "# Start with FJC dataframes\n",
    "dfs = {\n",
    "    # FJC dataframes\n",
    "    \"fjc_judges\": fjc_judges,\n",
    "    \"fjc_federal_judicial_service\": fjc_federal_judicial_service,\n",
    "    \"fjc_demographics\": fjc_demographics,\n",
    "    \"fjc_education\": fjc_education,\n",
    "    \"fjc_other_federal_judicial_service\": fjc_other_federal_judicial_service,\n",
    "    \"fjc_other_nominations_recess\": fjc_other_nominations_recess,\n",
    "    \"seat_timeline\": seat_timeline,\n",
    "    \n",
    "    # Congress raw dataframes\n",
    "    \"cong_nominations\": cong_nominations,\n",
    "    \"cong_nominees\": cong_nominees,\n",
    "    \n",
    "    # Exploded nomination dataframes\n",
    "    \"cong_nomination_core\": cong_nomination_core,\n",
    "    \"cong_nomination_nominees\": cong_nomination_nominees,\n",
    "    \"cong_nomination_actions\": cong_nomination_actions,\n",
    "    \"cong_nomination_committees\": cong_nomination_committees,\n",
    "    \"cong_nomination_hearings\": cong_nomination_hearings,\n",
    "    \n",
    "    # Exploded nominee dataframes\n",
    "    \"cong_nominee_core\": cong_nominee_core,\n",
    "    \"cong_nominee_orgs\": cong_nominee_orgs,\n",
    "    \"cong_nominee_edu\": cong_nominee_edu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of available dataframes\n",
    "print(\"Available dataframes:\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"- {name}: {len(df)} rows × {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4086e",
   "metadata": {},
   "source": [
    "JSON-containing files we can explode and/or flatten several different ways.  Whichever one is best depends on the use case.  Below is the method I settled on so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947abcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check uniqueness of 'citation' field in all dataframes\n",
    "from nomination_predictor.dataset import check_id_uniqueness\n",
    "\n",
    "print(\"Checking uniqueness of nomination/nominee identifiers...\")\n",
    "\n",
    "# Check each DataFrame for uniqueness of citation field\n",
    "for name, df in dfs.items():\n",
    "    if name.startswith(\"cong\"):\n",
    "        print(f\"\\n- Checking {name}...\")\n",
    "        if 'citation' in df.columns:\n",
    "            try:\n",
    "                check_id_uniqueness(df, id_field='citation')\n",
    "                print(f\"  ✅ Passed: 'citation' is unique in {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Failed: {e}\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Skipped: 'citation' column not found in {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87162502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented this cell out because IMO it's too early in this notebook to be worthwhile to save these as CSVs\n",
    "\n",
    "## Save extracted tables to interim directory\n",
    "for name, df in dfs.items():\n",
    "    if len(df) > 0:  # Only save non-empty DataFrames\n",
    "        output_path = INTERIM_DATA_DIR / f\"{name}.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved {len(df)} records to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682d66f",
   "metadata": {},
   "source": [
    "#### Quick peek at all loaded dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Checking for general shape and first handfuls of rows\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name:<35} → {df.shape}\")\n",
    "    print(df.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Checking for null values\")\n",
    "    \n",
    "for name, df in dfs.items():\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f541a22",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a3d80",
   "metadata": {},
   "source": [
    "## Normalize column names for DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0828397",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Column Names Before ===\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name:<35} → {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01227ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call features.py's normalize_columns function on all DataFrames in dfs, and strip leading and trailing whitespace in all strings\n",
    "from nomination_predictor.features import normalize_dataframe_columns\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df = normalize_dataframe_columns(df)\n",
    "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b405378",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Column Names After ===\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name:<35} → {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24052def",
   "metadata": {},
   "source": [
    "### Drop non-judge roles from nominations & nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-judicial nominations using the function from features.py\n",
    "from nomination_predictor.features import filter_non_judicial_nominations\n",
    "\n",
    "# Define non-judicial titles to filter out\n",
    "non_judicial_titles = [\n",
    "    \"Attorney\", \"Board\", \"Commission\", \"Director\", \"Marshal\",\n",
    "    \"Assistant\", \"Representative\", \"Secretary of\", \"Member of\"\n",
    "]\n",
    "\n",
    "# Apply the filter\n",
    "# FIXME: uncomment when we've downloaded enough nominees to be able to operate on\n",
    "#dfs[\"cong_nomination_nominations\"], dfs[\"cong_nominees\"] = filter_non_judicial_nominations(\n",
    "#    dfs[\"cong_nomination_nominations\"],\n",
    "#    dfs[\"cong_nominees\"],\n",
    "#    non_judicial_titles=non_judicial_titles\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b269eea",
   "metadata": {},
   "source": [
    "### Convert date strings to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01904b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for any columns which contain certain keywords in their column name and contain string values, convert from string to datetime\n",
    "datetime_related_keywords = (\"date\", \"year\", \"month\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col for keyword in datetime_related_keywords) and df[col].dtype == \"object\":\n",
    "            logger.info(f\"Converting {col} to datetime for {name}\")\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb8848",
   "metadata": {},
   "source": [
    "### Normalize court names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for any columns which contain certain keywords in their column name and contain string values, casefold and replace spans of one or more interstitial spaces with underscores\n",
    "court_describing_keywords = (\"court\", \"circuit\", \"district\")\n",
    "\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.casefold() for keyword in court_describing_keywords) and df[col].dtype == object:\n",
    "            logger.info(F\"Normalizing column named {col} in {name}\")\n",
    "            df[col] = df[col].str.casefold().str.replace(r'\\s+', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5071ca",
   "metadata": {},
   "source": [
    "### Count and display unique values under each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2df09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display counts of nique values in DataFrame columns:\n",
    "for name, df in dfs.items():\n",
    "    for col in sorted(df.columns):\n",
    "     print(f\"{name} - {col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba6742",
   "metadata": {},
   "source": [
    "### Set nid and citation as index for whichever dataframes intentionally use them uniquely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the dataframes that have unique nid, set them as the index to optimize lookups/joins\n",
    "dfs[\"fjc_judges\"].set_index('nid', drop=False, inplace=True, verify_integrity=True)\n",
    "dfs[\"fjc_demographics\"].set_index('nid', drop=False, inplace=True, verify_integrity=True)\n",
    "\n",
    "\n",
    "# congress' nominee dataframes are more consistent than the nominations dataframes about 'citation' field being unique\n",
    "for name, df in dfs.items():\n",
    "    if name.startswith(\"cong_nominees_\"):\n",
    "        logger.info(f\"Setting index for {name}\")\n",
    "        df.set_index('citation', drop=False, inplace=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558daca8",
   "metadata": {},
   "source": [
    "## Fuzzy-matching FJC judges to Congress.gov nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich the nominees dataframe with name fields and court information from nominations for the sake of the fuzzy-matcher\n",
    "from nomination_predictor.features import (enrich_congress_nominees_dataframe,\n",
    "                                           enrich_fjc_judges)\n",
    "\n",
    "dfs.cong_nominees = enrich_congress_nominees_dataframe(dfs.cong_nominees, dfs.cong_nominations)\n",
    "\n",
    "# Enrich the FJC judges dataframe with full name fields for the sake of the fuzzy-matcher\n",
    "fjc_combined = enrich_fjc_judges(fjc_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ef2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block by last name exact match\n",
    "blocks = {}\n",
    "for lname, group in fjc_judges.groupby(\"last\"):\n",
    "    blocks[lname] = group\n",
    "\n",
    "def candidate_fjc_rows(row):\n",
    "    return blocks.get(row[\"last\"], pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a767be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean Congress nominees ------------------------------------------------\n",
    "cong_nominees[\"full_name_clean\"] = cong_nominees[\"full_name\"].apply(clean_name)\n",
    "cong_nominees[[\"first\",\"middle\",\"last\"]] = cong_nominees[\"full_name_clean\"].apply(\n",
    "    lambda n: pd.Series(split_name(n)))\n",
    "\n",
    "cong_nominees[\"court_clean\"] = cong_nominees[\"organization\"].apply(normalised_court)\n",
    "cong_nominees[\"nomination_date\"] = pd.to_datetime(cong_nominees[\"nomination_date\"])\n",
    "\n",
    "# --- Clean FJC judges -------------------------------------------------------\n",
    "fjc_judges[\"full_name_clean\"] = fjc_judges[\"name_full\"].apply(clean_name)\n",
    "fjc_judges[[\"first\",\"middle\",\"last\"]] = fjc_judges[\"full_name_clean\"].apply(\n",
    "    lambda n: pd.Series(split_name(n)))\n",
    "\n",
    "# We'll need a mapping from nid to service records for date & court validation\n",
    "fjc_service[\"court_clean\"] = fjc_service[\"court_name\"].apply(normalised_court)\n",
    "fjc_service[\"nomination_date\"] = pd.to_datetime(fjc_service[\"nomination_date\"], errors=\"coerce\")\n",
    "fjc_service[\"commission_date\"] = pd.to_datetime(fjc_service[\"commission_date\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbc330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_match(row):\n",
    "    candidates = candidate_fjc_rows(row)\n",
    "    if candidates.empty:\n",
    "        return pd.NA, 0.0\n",
    "    # Compute combined score: name similarity + court similarity + date proximity\n",
    "    best_score = 0.0\n",
    "    best_nid = pd.NA\n",
    "    for _, cand in candidates.iterrows():\n",
    "        name_score = fuzz.token_set_ratio(row[\"full_name_clean\"], cand[\"full_name_clean\"])\n",
    "        # Use service records to find any matching nomination date\n",
    "        entries = fjc_service[fjc_service[\"nid\"] == cand[\"nid\"]]\n",
    "        date_score = 0\n",
    "        court_score = 0\n",
    "        if not entries.empty:\n",
    "            # Smallest absolute diff in days\n",
    "            diffs = (entries[\"nomination_date\"] - row[\"nomination_date\"]).abs().dt.days\n",
    "            date_score = 100 - diffs.min() if diffs.notna().any() else 0\n",
    "            # any court string overlap\n",
    "            if row[\"court_clean\"]:\n",
    "                if any(row[\"court_clean\"] in c for c in entries[\"court_clean\"]):\n",
    "                    court_score = 100\n",
    "                else:\n",
    "                    court_score = max(fuzz.partial_ratio(row[\"court_clean\"], c) for c in entries[\"court_clean\"])\n",
    "        total = 0.6*name_score + 0.3*date_score + 0.1*court_score\n",
    "        if total > best_score:\n",
    "            best_score, best_nid = total, cand[\"nid\"]\n",
    "    return best_nid, round(best_score,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new filter_confirmed_nominees function\n",
    "from nomination_predictor.features import (analyze_match_failures,\n",
    "                                           filter_confirmed_nominees,\n",
    "                                           load_simpler_dataframes)\n",
    "\n",
    "# Load and prepare all dataframes\n",
    "dfs = load_simpler_dataframes(RAW_DATA_DIR)\n",
    "cong_nominees = dfs[\"cong_nominees\"]  # This now has all the derived fields\n",
    "fjc_judges = dfs[\"fjc_judges\"]\n",
    "fjc_service = dfs[\"fjc_service\"]\n",
    "cong_nominations = dfs[\"cong_nominations\"]\n",
    "\n",
    "# OPTIMIZATION: Filter to only confirmed nominees before matching\n",
    "# This saves processing time by only matching nominees who were confirmed\n",
    "confirmed_nominees = filter_confirmed_nominees(cong_nominees, cong_nominations)\n",
    "print(f\"Focusing on {len(confirmed_nominees)} confirmed nominees out of {len(cong_nominees)} total nominees\")\n",
    "\n",
    "# Only apply best_match to confirmed nominees\n",
    "confirmed_nominees[[\"match_nid\", \"match_score\"]] = confirmed_nominees.apply(\n",
    "    best_match, axis=1, result_type=\"expand\")\n",
    "\n",
    "# Merge back with original dataframe to preserve all records\n",
    "# Non-confirmed nominees will have NaN for match fields\n",
    "cong_nominees = cong_nominees.merge(\n",
    "    confirmed_nominees[[\"citation\", \"match_nid\", \"match_score\"]], \n",
    "    on=\"citation\", \n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a709203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "THRESHOLD = 80\n",
    "matches = cong_nominees[cong_nominees[\"match_score\"] >= THRESHOLD].copy()\n",
    "print(f\"Matched {len(matches)}/{len(cong_nominees)} nominees with score ≥ {THRESHOLD}\")\n",
    "matches.to_csv(INTERIM_DATA_DIR / \"congress_fjc_nominee_matches.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: decide whether to save as separate vs. overwrite in interim folder\n",
    "## Save the cleaned interim datasets for downstream notebooks\n",
    "#cong_nominees.to_csv(INTERIM_DATA_DIR / \"congress_nominees_cleaned.csv\", index=False)\n",
    "#fjc_judges.to_csv(INTERIM_DATA_DIR / \"fjc_judges_cleaned.csv\", index=False)\n",
    "#fjc_service.to_csv(INTERIM_DATA_DIR / \"fjc_service_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomination_predictor.features import analyze_match_failures\n",
    "\n",
    "THRESHOLD = 80\n",
    "matches = cong_nominees[cong_nominees[\"match_score\"] >= THRESHOLD].copy()\n",
    "print(f\"Matched {len(matches)}/{len(cong_nominees)} nominees with score ≥ {THRESHOLD}\")\n",
    "\n",
    "# Analyze unmatched records to understand why they didn't match\n",
    "unmatched_df, reason_summary, examples = analyze_match_failures(cong_nominees, THRESHOLD)\n",
    "\n",
    "# Display summary of failure reasons\n",
    "print(\"\\nFailure Reason Summary:\")\n",
    "display(reason_summary)\n",
    "\n",
    "# Display a few examples of each failure type\n",
    "print(\"\\nExample records for each failure type:\")\n",
    "for reason, example_df in examples.items():\n",
    "    print(f\"\\n{reason}:\")\n",
    "    display(example_df)\n",
    "\n",
    "# Save both matched and unmatched datasets for further analysis\n",
    "matches.to_csv(INTERIM_DATA_DIR / \"congress_fjc_nominee_matches.csv\", index=False)\n",
    "unmatched_df.to_csv(INTERIM_DATA_DIR / \"congress_fjc_nominee_unmatched.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXME: decide whether to save as separate vs. overwrite in interim folder\n",
    "## Save the cleaned interim datasets for downstream notebooks\n",
    "#cong_nominees.to_csv(INTERIM_DATA_DIR / \"congress_nominees_cleaned.csv\", index=False)\n",
    "#fjc_judges.to_csv(INTERIM_DATA_DIR / \"fjc_judges_cleaned.csv\", index=False)\n",
    "#fjc_service.to_csv(INTERIM_DATA_DIR / \"fjc_service_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcfcbf",
   "metadata": {},
   "source": [
    "## Combining FJC data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7daa8b",
   "metadata": {},
   "source": [
    "### Handling nominees' education and job history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74656c7d",
   "metadata": {},
   "source": [
    "Before we combine FJC data, we have to consider whether/how to handle judges' education, job history, age, ABA rating, etc., because the only other table in the FJC data which handles nid uniquely is \"demographics,\" which are unchanging.\n",
    "The simplest way to handle the non-unique-nid tables it would be to left-merge on \"nid\" and only take the most recently-dated row.  In most cases this would likely land on keeping the most prestigious degree or job.\n",
    "\n",
    "However, it is entirely likely a judge's education or job history has changed substantially since their first nomination, and affected their qualifications for each later nomination.\n",
    "\n",
    "All of these indicate to me that it's worth considering the judge's position, education, etc., not as of the most recent records available, but instead _as of when they were nominated._\n",
    "\n",
    "That means we can't do a simple left-join of all of our FJC data.  Instead, we have to -- using a combination of names, court locations, and vacancy dates -- fuzzy-match to find which \"nid\" corresponds to each \"citation\" in the Congress data, as our way of bridging between FJC judges and congress' nominee data. Then use the \"received date\" for that citation as a cutoff date for when we lookup education and job records by \"nid\" -- so we can avoid mistakenly linking to a citation any employemnt & job records dated after that cutoff date.\n",
    "\n",
    "Thankfully we do have the school, degree, and degree_year in the education record, for both their bachelors and their masters and their associate degree(s) and LLB and J.D. etc., so we can look that up.  The education dataframe even comes with a \"sequence\" number for each education record, which is an even easier-to-use indicator of chronological order than the degree_year for any given \"nid\" lookup for a judge.\n",
    "\n",
    "Job history is more challenging to deal with because literally every row entry in that dataframe lists it uniquely, but we do have the data available.  On early attempts, it may be simplest to ignore it; then feature-engineer basic booleans for whether they did/didn't have experience in common-phrase-identifiable positions such as \"Private practice\" or \"Attorney general\" or \"Navy\" or \"Army\" etc.; eventually a parser can look for the year spreads listed there as a rough indicator of amounts of experience gleaned from each professional role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ea206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left-joins all dataframes whose names start with \"fjc\", joining them on their columns named \"nid\"\n",
    "# Warns if any shared column names contain non-identical data\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from nomination_predictor.features import left_join_fjc_dataframes\n",
    "\n",
    "# Execute the function with our dataframes\n",
    "try:\n",
    "    fjc_combined = left_join_fjc_dataframes(dfs)\n",
    "    \n",
    "    if fjc_combined is not None:\n",
    "        logger.info(f\"Successfully created combined FJC dataframe with {len(fjc_combined)} rows and {len(fjc_combined.columns)} columns\")\n",
    "        # Display the first few rows of the result\n",
    "        fjc_combined.head()\n",
    "    else:\n",
    "        logger.error(\"Failed to create combined FJC dataframe\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error joining FJC dataframes: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd9fe3",
   "metadata": {},
   "source": [
    "### Build predecessor lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predecessor lookup table\n",
    "predecessor_lookup = get_predecessor_info(seat_timeline_df)\n",
    "print(f\"Created predecessor lookup: {len(predecessor_lookup)} records\")\n",
    "\n",
    "# Preview the predecessor lookup\n",
    "print(predecessor_lookup.head())\n",
    "all_dataframes['predecessor_lookup'] = predecessor_lookup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomination_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
